import math
import time
import os
import re
import requests
from bs4 import BeautifulSoup, NavigableString
import pandas as pd
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, Optional, List


# -------------------------------
# Saramin Crawler (Í≤ÄÏÉâ + ÏÉÅÏÑ∏ ÌååÏã±: AÎ™®Îìú=ÏõêÎ¨∏ Ï†ïÎ¶¨ + ÏöîÏïΩ)
# -------------------------------
class SaraminCrawler:
    def __init__(self):
        self.api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.saramin.co.kr/zf_user/search",
            "X-Requested-With": "XMLHttpRequest",
        }
        # üîé Í≤ÄÏÉâ Ï°∞Í±¥(ÏöîÏ≤≠ Ï°∞Í±¥)
        self.params = {
            "searchType": "search",
            "loc_mcd": "106000,104000,105000,107000,110000,111000",   # Î∂ÄÏÇ∞/ÎåÄÍµ¨/ÎåÄÏ†Ñ/Ïö∏ÏÇ∞/Í≤ΩÎÇ®/Í≤ΩÎ∂Å
            "cat_kewd": "83,84,85,90,104,108,111,112,114,116",       # Îç∞Ïù¥ÌÑ∞ÏóîÏßÄÎãàÏñ¥ Ïô∏ 10Í∞ú
            "company_cd": "0,1,2,3,4,5,6,7,9,10",                    # ÌöåÏÇ¨ÌòïÌÉú Ï†ÑÏ≤¥
            "exp_cd": "1",                                           # Ïã†ÏûÖ
            "exp_none": "y",                                         # Í≤ΩÎ†•Î¨¥Í¥Ä Ìè¨Ìï®
            "job_type": "1",                                         # Ï†ïÍ∑úÏßÅ
            "search_optional_item": "y",
            "search_done": "y",
            "panel_count": "y",
            "preview": "y",
            "recruitPage": 1,
            "recruitPageCount": 40,                                  # ÌéòÏù¥ÏßÄÎãπ 40Í∞ú
            "recruitSort": "relation"                                # Í¥ÄÎ†®ÎèÑÏàú
        }

        # Î∂àÌïÑÏöî ÌÖçÏä§Ìä∏ Ï†úÍ±∞ Ìå®ÌÑ¥(ÏÉÅÏÑ∏ Î≥∏Î¨∏ ÌÅ¥Î¶∞ÏóÖ)
        self.noise_patterns = [
            r"\bÎ°úÍ∑∏Ïù∏\b", r"\bÌöåÏõêÍ∞ÄÏûÖ\b", r"Í∏∞ÏóÖÏÑúÎπÑÏä§", r"ÏÇ¨ÎûåÏù∏\s*ÎπÑÏ¶àÎãàÏä§", r"ÏÇ¨ÎûåÏù∏\s*Í≥†Í∞ùÏÑºÌÑ∞",
            r"\bTOP\b", r"\bÏù¥Ï†ÑÍ≥µÍ≥†\b", r"\bÎã§ÏùåÍ≥µÍ≥†\b", r"Í≤ÄÏÉâ\s*Ìèº", r"Í≥µÏßÄÏÇ¨Ìï≠", r"Ïù¥Î≤§Ìä∏",
            r"Ïª§Î¶¨Ïñ¥\s*ÌîºÎìú", r"ÏÇ¨ÎûåÏù∏\s*Ïä§ÌÜ†Ïñ¥", r"Ï±ÑÏö©Ï†ïÎ≥¥\s*ÏßÄÏó≠Î≥Ñ", r"HOT100", r"Ìó§ÎìúÌóåÌåÖ", r"ÌÅêÎ†àÏù¥ÏÖò",
            r"ÌååÍ≤¨ÎåÄÌñâ", r"Ïô∏Íµ≠Ïù∏\s*Ï±ÑÏö©", r"Ï§ëÏû•ÎÖÑ\s*Ï±ÑÏö©", r"Ï∑®ÏóÖÏ∂ïÌïòÍ∏à", r"Ïã†ÏûÖ¬∑Ïù∏ÌÑ¥", r"Ï±ÑÏö©Îã¨Î†•",
            r"Ïó∞Î¥âÏ†ïÎ≥¥", r"Î©¥Ï†ëÌõÑÍ∏∞", r"Í∏∞ÏóÖÌÅêÎ†àÏù¥ÏÖò", r"Ïù¥Î†•ÏÑú\s*ÏñëÏãù", r"HRÎß§Í±∞ÏßÑ",
            r"Ïù∏Ï†ÅÏÑ±Í≤ÄÏÇ¨", r"ÏÑúÎ•ò\s*ÏûëÏÑ±\s*ÏΩîÏπ≠", r"Î©¥Ï†ë\s*ÏΩîÏπ≠", r"ÏûêÍ∏∞\s*Í≥ÑÎ∞ú", r"ÏûêÍ≤©Ï¶ù\s*Ï§ÄÎπÑ",
            r"ÏÇ¨ÎûåÏù∏\s*Ïù∏Í≥µÏßÄÎä•", r"ÎßûÏ∂§\s*Í≥µÍ≥†", r"Ai\s*Îß§Ïπò", r"Ai\s*Î™®ÏùòÎ©¥Ï†ë"
        ]
        self.noise_regex = re.compile("|".join(self.noise_patterns))

    # ---------- Ïú†Ìã∏ ----------
    @staticmethod
    def _clean_ws(text: str) -> str:
        if not text:
            return ""
        t = text.replace("\r", "\n")
        t = re.sub(r"\u00A0", " ", t)  # non-breaking space
        t = re.sub(r"[ \t]+", " ", t)
        t = re.sub(r"\n{2,}", "\n", t)
        return t.strip()

    def _is_noise_line(self, line: str) -> bool:
        if not line:
            return True
        if len(line) < 2:
            return True
        if self.noise_regex.search(line):
            return True
        # ÏßÄÎÇòÏπòÍ≤å Î©îÎâ¥ÏÑ± ÎÇòÏó¥
        if sum(1 for ch in line if ch == "¬∑" or ch == "|") >= 3:
            return True
        return False

    def _dedup_lines_ordered(self, lines: List[str]) -> List[str]:
        seen = set()
        out = []
        for ln in lines:
            k = ln.strip()
            if not k or k in seen:
                continue
            seen.add(k)
            out.append(k)
        return out

    # ---------- Í≤ÄÏÉâÍ≤∞Í≥º ÌååÏã± ----------
    def _parse_jobs_from_innerHTML(self, inner_html):
        soup = BeautifulSoup(inner_html, "html.parser")
        jobs = []
        for item in soup.select("div.item_recruit"):
            try:
                rec_idx = (item.get("value") or "").strip()
                a = item.select_one("h2.job_tit a")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = "https://www.saramin.co.kr" + href if href.startswith("/") else href

                company_el = item.select_one("strong.corp_name a, strong.corp_name")
                company = company_el.get_text(strip=True) if company_el else ""

                cond_spans = item.select("div.job_condition span")
                location = cond_spans[0].get_text(strip=True) if len(cond_spans) > 0 else ""
                career   = cond_spans[1].get_text(strip=True) if len(cond_spans) > 1 else ""
                edu      = cond_spans[2].get_text(strip=True) if len(cond_spans) > 2 else ""

                deadline_el = item.select_one("div.job_date span.date")
                deadline = deadline_el.get_text(strip=True) if deadline_el else ""

                jobs.append({
                    "rec_idx": rec_idx,
                    "title": title,
                    "company": company,
                    "location": location,
                    "career": career,
                    "education": edu,
                    "deadline": deadline,
                    "link": link,
                    "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception:
                continue
        return jobs

    def _fetch_page(self, page: int):
        params = dict(self.params)
        params["recruitPage"] = page
        resp = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        inner_html = data.get("innerHTML", "")
        count_str = data.get("count", "0")
        try:
            total_count = int(str(count_str).replace(",", ""))
        except Exception:
            total_count = 0

        jobs = self._parse_jobs_from_innerHTML(inner_html) if inner_html else []
        return jobs, total_count

    def crawl_all(self, sleep_sec=0.6, page_limit=None) -> pd.DataFrame:
        print("üîé ÏàòÏßë ÏãúÏûë...")
        all_jobs = []
        first_page_jobs, total_count = self._fetch_page(1)
        if not first_page_jobs:
            print("‚ö† Ï≤´ ÌéòÏù¥ÏßÄÏóêÏÑú Í≥µÍ≥†Î•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§. (Ìó§Îçî/ÌååÎùºÎØ∏ÌÑ∞ ÌôïÏù∏ ÌïÑÏöî)")
            return pd.DataFrame()

        all_jobs.extend(first_page_jobs)

        page_count = math.ceil(total_count / int(self.params["recruitPageCount"])) if total_count else 1
        if page_limit is not None:
            page_count = min(page_count, page_limit)

        print(f"üìä Ï¥ù {total_count}Í±¥ Ï∂îÏ†ï, {page_count}ÌéòÏù¥ÏßÄ ÏòàÏ†ï")

        for p in range(2, page_count + 1):
            print(f"üìÑ {p}/{page_count} ÌéòÏù¥ÏßÄ ÏàòÏßë Ï§ë...")
            jobs, _ = self._fetch_page(p)
            if not jobs:
                print("‚õî Îçî Ïù¥ÏÉÅ Í≥µÍ≥†Í∞Ä ÏóÜÏäµÎãàÎã§.")
                break
            all_jobs.extend(jobs)
            time.sleep(sleep_sec)

        df = pd.DataFrame(all_jobs)
        if df.empty:
            print("‚ö† ÏàòÏßëÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return df

        # Ï§ëÎ≥µ Ï†úÍ±∞: rec_idx Ïö∞ÏÑ†, ÏóÜÏúºÎ©¥ link
        if "rec_idx" in df.columns:
            df["__dedup_key"] = df["rec_idx"].where(df["rec_idx"].astype(bool), other=df["link"])
            df.drop_duplicates(subset=["__dedup_key"], inplace=True)
            df.drop(columns=["__dedup_key"], inplace=True)
        else:
            df.drop_duplicates(subset=["link"], inplace=True)

        return df

    # ---------- ÏÉÅÏÑ∏ÌéòÏù¥ÏßÄ ÌååÏã± (A Î™®Îìú: ÏõêÎ¨∏+Ï†ïÏ†ú) ----------
    def _find_content_container(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        """
        ÏÉÅÏÑ∏ Î≥∏Î¨∏Ïù¥ Îì§Ïñ¥ÏûàÎäî ÏµúÏÉÅÏúÑ Ïª®ÌÖåÏù¥ÎÑàÎ•º ÏµúÎåÄÌïú Ï†ïÌôïÌûà ÏÑ†ÌÉù
        """
        selectors = [
            "#content .wrap_jview",
            "#content .jv-cont",
            "#content .jv_detail",
            "#content .cont",
            "#recruit_info",
            ".wrap_jv_cont",
            ".jview .cont",
            "#content"
        ]
        for sel in selectors:
            node = soup.select_one(sel)
            if node and len(node.get_text(strip=True)) > 50:
                return node
        return soup  # fallback

    def _strip_noise_nodes(self, node: BeautifulSoup):
        """
        Î≥∏Î¨∏ Ïô∏ Ïû°ÏòÅÏó≠ Ï†úÍ±∞
        """
        for tag in node.find_all(["script", "style", "noscript", "iframe"]):
            tag.decompose()
        for tag in node.find_all(["header", "footer", "nav", "aside"]):
            tag.decompose()
        # Î©îÎâ¥/Í¥ëÍ≥†ÏÑ± ÏÑπÏÖòÎì§ Ï∂îÏ†ï ÌÅ¥ÎûòÏä§ Ï†úÍ±∞
        junk_classes = [
            "gnb", "lnb", "breadcrumb", "sidebar", "banner", "ad", "advert", "footer",
            "login", "signup", "jv-relate", "sns", "share", "floating", "btn_top"
        ]
        for cls in junk_classes:
            for t in node.select(f".{cls}"):
                t.decompose()

    def _extract_label_value(self, soup: BeautifulSoup, labels: List[str]) -> Optional[str]:
        """
        ÎùºÎ≤®-Í∞í Íµ¨Ï°∞ Ï∂îÏ∂ú (dt/dd, th/td, strong/label Îì±)
        """
        nodes = soup.find_all(string=re.compile("|".join([re.escape(x) for x in labels])))
        for node in nodes:
            parent = node.parent
            if not parent:
                continue
            # dt -> dd
            if parent.name == "dt":
                dd = parent.find_next_sibling("dd")
                if dd:
                    return self._clean_ws(dd.get_text(" ", strip=True))
            # th -> td
            if parent.name == "th":
                td = parent.find_next_sibling("td")
                if td:
                    return self._clean_ws(td.get_text(" ", strip=True))
            # strong/label Î∞îÎ°ú Îã§Ïùå ÌòïÏ†ú
            sib = parent.find_next_sibling()
            if sib and sib.name in ["dd", "td", "p", "div", "span"]:
                val = self._clean_ws(sib.get_text(" ", strip=True))
                if val:
                    return val
            # Í∞ôÏùÄ Ï§ÑÏóêÏÑú ÏΩúÎ°† Îì±ÏúºÎ°ú Ïù¥Ïñ¥ÏßÑ ÏºÄÏù¥Ïä§
            line = self._clean_ws(parent.get_text(" ", strip=True))
            for kw in labels:
                if kw in line:
                    after = line.split(kw, 1)[1].lstrip(": -‚Äî\t")
                    if after:
                        return self._clean_ws(after)
        return None

    def _extract_section_text(self, node: BeautifulSoup, title_patterns: List[str]) -> Optional[str]:
        """
        ÏÑπÏÖò(ÏûêÍ≤©ÏöîÍ±¥/Î≥µÎ¶¨ÌõÑÏÉù Îì±)ÏùÑ Ï†ïÏ†ú ÌÖçÏä§Ìä∏Î°ú Ï∂îÏ∂ú
        - ÌÉÄÏù¥ÌãÄÍ≥º Í∞ÄÍπåÏö¥ DOMÏùÑ Ïö∞ÏÑ† Ï∂îÏ†Å
        - li/p/div/span ÌÖçÏä§Ìä∏ Ï∑®Ìï©
        - ÎÖ∏Ïù¥Ï¶à ÎùºÏù∏ Ï†úÍ±∞, Ï§ëÎ≥µ Ï†úÍ±∞
        """
        regex = re.compile("|".join(title_patterns), re.IGNORECASE)
        hits = node.find_all(string=regex)
        candidates = []

        for hit in hits:
            box = hit
            # ÏÉÅÏúÑÎ°ú 2~3Îã®Í≥Ñ Ïò¨Î†§ ÏÑπÏÖò ÎûòÌçº Ï∂îÏ†ï
            for _ in range(3):
                if box and getattr(box, "parent", None):
                    box = box.parent
            if not box:
                continue

            texts = []
            for t in box.find_all(["li", "p", "dd", "td", "div", "span"]):
                s = t.get_text(" ", strip=True)
                s = self._clean_ws(s)
                if s:
                    texts.append(s)

            if not texts:
                # Ïù∏Ï†ë ÌòïÏ†úÏóêÏÑú ÏùºÏ†ï Í∞úÏàò Ï∂îÏ∂ú
                sibs = []
                for sib in box.find_all_next(["li", "p", "dd", "td", "div", "span"], limit=60):
                    txt = self._clean_ws(sib.get_text(" ", strip=True))
                    if txt:
                        sibs.append(txt)
                texts = sibs

            # ÎÖ∏Ïù¥Ï¶à ÌïÑÌÑ∞
            texts = [ln for ln in texts if not self._is_noise_line(ln)]
            # ÎÑàÎ¨¥ Í∏¥ Ï§ÑÏùÄ Î¨∏Ïû• Îã®ÏúÑÎ°ú ÏûêÎ•¥Í∏∞
            refined = []
            for ln in texts:
                if len(ln) > 300:
                    parts = re.split(r"[‚Ä¢¬∑\-\u2022\|\n]+", ln)
                    for p in parts:
                        p = self._clean_ws(p)
                        if p and not self._is_noise_line(p):
                            refined.append(p)
                else:
                    refined.append(ln)

            refined = self._dedup_lines_ordered(refined)
            if refined:
                candidates.append("\n".join(refined))

        if candidates:
            raw = max(candidates, key=len)  # Í∞ÄÏû• Í∏¥ Í≤ÉÏùÑ ÏÑπÏÖòÏúºÎ°ú Í∞ÑÏ£º
            raw = self._clean_ws(raw[:8000])  # ÏïàÏ†Ñ Í∏∏Ïù¥ Ï†úÌïú
            # ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú Î©îÎâ¥ÏÑ±/Í¥ëÍ≥†ÏÑ± ÎùºÏù∏ Ï∂îÍ∞Ä ÌïÑÌÑ∞ÎßÅ
            final_lines = [ln for ln in raw.split("\n") if not self._is_noise_line(ln)]
            final = "\n".join(self._dedup_lines_ordered(final_lines))
            return final if final.strip() else None

        return None

    def _summarize_bullets(self, text: str, max_lines: int = 5) -> str:
        """
        Í∞ÑÎã® Í∑úÏπô Í∏∞Î∞ò ÏöîÏïΩ:
        - Î∂àÎ¶ø/Ïà´Ïûê/ÌÇ§ÏõåÎìú Ìè¨Ìï® ÎùºÏù∏ ÏúÑÏ£º ÏÑ†Î≥Ñ
        - Ï†ÑÎ¨∏/Í¥ëÍ≥†ÏÑ± Î¨∏Íµ¨ Ï†úÍ±∞
        """
        if not text:
            return ""

        lines = [self._clean_ws(x) for x in text.split("\n")]
        lines = [x for x in lines if x and not self._is_noise_line(x)]

        # ÌÇ§ÏõåÎìú Ï†êÏàò Í∏∞Î∞ò ÏÑ†Î≥Ñ
        keywords = [
            "ÌïÑÏàò", "Ïö∞ÎåÄ", "Í≤ΩÎ†•", "Ïã†ÏûÖ", "Í∞úÎ∞ú", "Ïö¥ÏòÅ", "Python", "Java", "C++", "ÏÑúÎ≤Ñ", "DB",
            "AWS", "OCI", "Linux", "ÎÑ§Ìä∏ÏõåÌÅ¨", "Î≥¥Ïïà", "ÏûêÍ≤©", "ÌïôÎ†•", "Ï†ÑÍ≥µ", "Ïö∞ÎåÄÏÇ¨Ìï≠", "Îã¥ÎãπÏóÖÎ¨¥",
            "Í∑ºÎ¨¥", "Í∑ºÎ¨¥ÏßÄ", "Ïó∞Î¥â", "ÌòëÏùò", "Ï†ïÍ∑úÏßÅ", "Î≥µÎ¶¨ÌõÑÏÉù", "4ÎåÄÎ≥¥Ìóò", "ÏãùÎåÄ", "Ïó∞Ï∞®", "Ìá¥ÏßÅÍ∏à",
            "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§", "ÏûêÍ≤©Ï¶ù", "Ï†ïÎ≥¥Ï≤òÎ¶¨", "Ï†ÑÎ¨∏Ïó∞", "Î≥ëÏó≠", "Íµ∞ÌïÑ"
        ]
        kw_re = re.compile("|".join([re.escape(k) for k in keywords]), re.IGNORECASE)

        scored = []
        for ln in lines:
            score = 0
            if re.search(r"^[\-\‚Ä¢\u2022\*\d]+\s", ln):  # Î∂àÎ¶ø/Ïà´Ïûê ÏãúÏûë
                score += 2
            if len(ln) <= 140:
                score += 1
            if kw_re.search(ln):
                score += 3
            scored.append((score, ln))

        scored.sort(key=lambda x: (-x[0], len(x[1])))  # Ï†êÏàò‚Üì, Í∏∏Ïù¥‚Üë
        picked = []
        used = set()
        for _, ln in scored:
            if ln in used:
                continue
            used.add(ln)
            picked.append(ln)
            if len(picked) >= max_lines:
                break

        if not picked:
            # fallback: ÏïûÎ∂ÄÎ∂Ñ ÏÉÅÏúÑ 3~5Ï§Ñ
            picked = lines[:max_lines]

        # Î∂àÎ¶ø Ìè¨Îß∑ÏúºÎ°ú Ï†ïÎ¶¨
        return "‚Ä¢ " + "\n‚Ä¢ ".join([self._clean_ws(x) for x in picked])

    def _fetch_and_parse_detail(self, session: requests.Session, url: str) -> Tuple[str, Dict[str, str]]:
        """
        ÏÉÅÏÑ∏ÌéòÏù¥ÏßÄ 1Í±¥ ÏöîÏ≤≠+ÌååÏã±. (ÏÑ∏ÏÖò/ÌÉÄÏûÑÏïÑÏõÉ/Î¶¨Ìä∏ÎùºÏù¥ ÎÇ¥Ïû•)
        Î∞òÌôò: (url, {employment_type, salary, requirements_raw, benefits_raw, requirements_summary, benefits_summary})
        """
        result = {
            "employment_type": "", "salary": "",
            "requirements_raw": "", "benefits_raw": "",
            "requirements_summary": "", "benefits_summary": ""
        }
        if not url:
            return url, result

        for _ in range(3):
            try:
                resp = session.get(url, timeout=20, headers=self.headers)
                if resp.status_code != 200:
                    time.sleep(0.4)
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")

                content = self._find_content_container(soup)
                self._strip_noise_nodes(content)

                # ÎùºÎ≤® Í∏∞Î∞ò (Í≥†Ïö©ÌòïÌÉú/Í∏âÏó¨)
                emp = self._extract_label_value(content, ["Í≥†Ïö©ÌòïÌÉú", "Í∑ºÎ¨¥ÌòïÌÉú"])
                sal = self._extract_label_value(content, ["Í∏âÏó¨", "Ïó∞Î¥â", "Î≥¥Ïàò", "Í∏âÏó¨Ï°∞Í±¥"])

                # ÏÑπÏÖò Í∏∞Î∞ò (ÏûêÍ≤©ÏöîÍ±¥/Î≥µÎ¶¨ÌõÑÏÉù)
                req = self._extract_section_text(
                    content,
                    ["ÏûêÍ≤©ÏöîÍ±¥", "ÏßÄÏõêÏûêÍ≤©", "ÌïÑÏàòÏöîÍ±¥", "Ïö∞ÎåÄÏÇ¨Ìï≠", "Ïö∞ÎåÄÏ°∞Í±¥", "Î™®ÏßëÏöîÍ∞ï", "Îã¥ÎãπÏóÖÎ¨¥"]
                )
                ben = self._extract_section_text(
                    content,
                    ["Î≥µÎ¶¨ÌõÑÏÉù", "ÌòúÌÉù", "ÏßÄÏõêÏ†úÎèÑ", "ÌöåÏÇ¨Î≥µÏßÄ"]
                )

                # ÏöîÏïΩ ÏÉùÏÑ±
                req_sum = self._summarize_bullets(req or "", max_lines=5) if req else ""
                ben_sum = self._summarize_bullets(ben or "", max_lines=4) if ben else ""

                result.update({
                    "employment_type": emp or "",
                    "salary": sal or "",
                    "requirements_raw": req or "",
                    "benefits_raw": ben or "",
                    "requirements_summary": req_sum,
                    "benefits_summary": ben_sum
                })
                return url, result
            except Exception:
                time.sleep(0.6)
                continue

        return url, result  # Ïã§Ìå® Ïãú Îπà Í∞í

    def enrich_with_details(self, df: pd.DataFrame, max_workers: int = 8) -> pd.DataFrame:
        """
        Î©ÄÌã∞Ïä§Î†àÎìúÎ°ú ÏÉÅÏÑ∏ÌéòÏù¥ÏßÄÎ•º Î≥ëÎ†¨ ÌååÏã±ÌïòÏó¨ Ïª¨Îüº Ï∂îÍ∞Ä (ÏõêÎ¨∏ + ÏöîÏïΩ)
        """
        if df.empty:
            return df

        urls = df["link"].fillna("").tolist()
        results_map: Dict[str, Dict[str, str]] = {}

        with requests.Session() as session:
            session.headers.update(self.headers)
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = {ex.submit(self._fetch_and_parse_detail, session, url): url for url in urls}
                for fut in as_completed(futures):
                    url, parsed = fut.result()
                    results_map[url] = parsed

        for col in [
            "employment_type", "salary",
            "requirements_raw", "benefits_raw",
            "requirements_summary", "benefits_summary"
        ]:
            df[col] = df["link"].map(lambda u: results_map.get(u, {}).get(col, ""))

        return df

    # ---------- HTML/Ïù¥Î©îÏùº ----------
    def build_html_page(self, df: pd.DataFrame, out_html_path: str, page_title: str = "Ï±ÑÏö©Í≥µÍ≥† Í≤∞Í≥º(Ï†ïÏ†ú+ÏöîÏïΩ)"):
        out_path = Path(out_html_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        cols = [
            'title','company','location','career','education','deadline',
            'employment_type','salary',
            'requirements_summary','benefits_summary',
            'requirements_raw','benefits_raw',
            'link','crawled_at'
        ]
        exist_cols = [c for c in cols if c in df.columns]
        styled = df[exist_cols].rename(columns={
            'title':'Ï†úÎ™©','company':'ÌöåÏÇ¨','location':'ÏúÑÏπò','career':'Í≤ΩÎ†•',
            'education':'ÌïôÎ†•','deadline':'ÎßàÍ∞êÏùº','employment_type':'Í≥†Ïö©ÌòïÌÉú',
            'salary':'Í∏âÏó¨','requirements_summary':'ÏûêÍ≤©ÏöîÍ±¥(ÏöîÏïΩ)','benefits_summary':'Î≥µÎ¶¨ÌõÑÏÉù(ÏöîÏïΩ)',
            'requirements_raw':'ÏûêÍ≤©ÏöîÍ±¥(ÏõêÎ¨∏)','benefits_raw':'Î≥µÎ¶¨ÌõÑÏÉù(ÏõêÎ¨∏)',
            'link':'ÎßÅÌÅ¨','crawled_at':'ÏàòÏßëÏãúÍ∞Å'
        }).copy()

        # ÎßÅÌÅ¨ Ïª¨Îüº HTMLÎ°ú Î≥ÄÌôò
        if 'ÎßÅÌÅ¨' in styled.columns:
            styled['ÎßÅÌÅ¨'] = styled['ÎßÅÌÅ¨'].apply(lambda x: f'<a href="{x}" target="_blank">Î∞îÎ°úÍ∞ÄÍ∏∞</a>' if x else '')

        # ÏõêÎ¨∏ÏùÄ Ï§ÑÎ∞îÍøà Î≥¥Ï°¥
        for c in ['ÏûêÍ≤©ÏöîÍ±¥(ÏõêÎ¨∏)', 'Î≥µÎ¶¨ÌõÑÏÉù(ÏõêÎ¨∏)', 'ÏûêÍ≤©ÏöîÍ±¥(ÏöîÏïΩ)', 'Î≥µÎ¶¨ÌõÑÏÉù(ÏöîÏïΩ)']:
            if c in styled.columns:
                styled[c] = styled[c].astype(str).str.replace("\n", "<br>")

        table_html = styled.to_html(index=False, escape=False, justify="center", border=0)
        html = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>{page_title}</title>
<style>
body {{ font-family: -apple-system, BlinkMacSystemFont, "Apple SD Gothic Neo", Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }}
.container {{ max-width:1200px; margin:0 auto; }}
h1 {{ font-size:24px; margin-bottom:8px; }}
.desc {{ color:#666; margin-bottom:20px; }}
table {{ width:100%; border-collapse:collapse; table-layout:fixed; word-break:break-word; }}
th, td {{ border-bottom:1px solid #eee; padding:12px 8px; text-align:left; vertical-align:top; }}
th {{ background:#fafafa; font-weight:700; }}
tr:hover {{ background:#f4f9ff; }}
a {{ text-decoration:none; color:#3498db; }}
.meta {{ color:#888; font-size:13px; margin-top:20px; }}
.small {{ color:#888; font-size:12px; }}
</style>
</head>
<body>
<div class="container">
  <h1>{page_title}</h1>
  <div class="desc">ÏÉùÏÑ± ÏãúÍ∞Å: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
  <p class="small">* Î©îÏùºÏóêÎäî ÏöîÏïΩ ÏúÑÏ£ºÎ°ú ÌëúÏãúÎêòÎ©∞, Î≥∏ ÌéòÏù¥ÏßÄÏóêÏÑúÎäî ÏõêÎ¨∏Í≥º ÏöîÏïΩÏù¥ Ìï®Íªò Ï†úÍ≥µÎê©ÎãàÎã§.</p>
  {table_html}
  <div class="meta">Powered by Python ¬∑ ÏûêÎèô ÌÅ¨Î°§ÎßÅ ¬∑ Updated everyday</div>
</div>
</body>
</html>"""
        out_path.write_text(html, encoding="utf-8")
        print(f"üåê HTML ÏÉùÏÑ±: {out_path}")
        return str(out_path)

    def generate_html_table_for_email(self, df: pd.DataFrame, max_rows=10):
        subset = df.head(max_rows).fillna("")
        cols = [
            "title","company","location","employment_type","salary",
            "requirements_summary","benefits_summary"
        ]
        exist = [c for c in cols if c in subset.columns]
        th_map = {
            "title":"Ï†úÎ™©","company":"ÌöåÏÇ¨","location":"ÏúÑÏπò",
            "employment_type":"Í≥†Ïö©ÌòïÌÉú","salary":"Í∏âÏó¨",
            "requirements_summary":"ÏûêÍ≤©ÏöîÍ±¥(ÏöîÏïΩ)","benefits_summary":"Î≥µÎ¶¨ÌõÑÏÉù(ÏöîÏïΩ)"
        }
        thead = "".join([f"<th>{th_map.get(c,c)}</th>" for c in exist])
        html = "<table style='width:100%; border-collapse:collapse; margin-top:20px;'>"
        html += f"<tr style='background:#667eea; color:white;'>{thead}</tr>"
        for _, row in subset.iterrows():
            html += "<tr>"
            for c in exist:
                val = str(row.get(c, "")).replace("\n", "<br>")
                html += f"<td style='padding:8px 6px; border-bottom:1px solid #eee;'>{val}</td>"
            html += "</tr>"
        html += "</table>"
        return html

    def send_email(self, df: pd.DataFrame, sender_email, app_password, receiver_email, pages_url: str):
        """
        CSV Ï≤®Î∂Ä ÏóÜÏù¥ HTML Î≥∏Î¨∏ + Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄ ÎßÅÌÅ¨Îßå Ï†ÑÏÜ°
        """
        if df.empty:
            print("‚ö† Ï†ÑÏÜ°Ìï† Í≥µÍ≥†Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return

        subject = f"üéØ Ï±ÑÏö©Í≥µÍ≥† ÏûêÎèô ÏàòÏßë Í≤∞Í≥º(Ï†ïÏ†ú+ÏöîÏïΩ) - {datetime.now().strftime('%Y-%m-%d')}"
        html_table = self.generate_html_table_for_email(df, max_rows=10)

        html_body = f"""
        <html><head><meta charset="UTF-8"></head>
        <body style="font-family:'Apple SD Gothic Neo',Arial,sans-serif;">
        <h1>üéØ Ï±ÑÏö©Í≥µÍ≥† ÏûêÎèô ÏàòÏßë Í≤∞Í≥º (Ï†ïÏ†ú+ÏöîÏïΩ)</h1>
        <p>{datetime.now().strftime('%YÎÖÑ %mÏõî %dÏùº')} ÏàòÏßë ÏôÑÎ£å</p>
        <div>
          <h2>üìä ÏàòÏßë ÌòÑÌô©</h2>
          <p>‚Ä¢ <strong>Ï¥ù {len(df)}Í∞ú</strong> Í≥µÍ≥† Î∞úÍ≤¨</p>
        </div>
        <div>
          <h2>üî• Ï£ºÏöî Í≥µÍ≥† ÎØ∏Î¶¨Î≥¥Í∏∞ (ÏµúÎåÄ 10Í∞ú, ÏöîÏïΩ)</h2>
          {html_table}
        </div>
        <div style="text-align:center; margin:30px 0;">
          <a href="{pages_url}" style="display:inline-block; padding:12px 20px; background:#3498db; color:#fff; text-decoration:none; border-radius:6px;">üåê Ï†ÑÏ≤¥ Í≥µÍ≥† Î≥¥Í∏∞ (ÏõêÎ¨∏+ÏöîÏïΩ)</a>
        </div>
        <p style="font-size:12px; color:#888;">ü§ñ CSV ÌååÏùºÏùÄ Ìè¨Ìï®ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Îäî ÏÉÅÎã® ÎßÅÌÅ¨ÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìï©ÎãàÎã§.</p>
        </body></html>
        """

        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(html_body, 'html', 'utf-8'))

        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, app_password)
            server.send_message(msg)
            server.quit()
            print("üìß Ïù¥Î©îÏùº Ï†ÑÏÜ° ÏôÑÎ£å! (Ï≤®Î∂Ä ÏóÜÏùå)")
        except Exception as e:
            print(f"‚ùå Ïù¥Î©îÏùº Ï†ÑÏÜ° Ïã§Ìå®: {e}")


# -------------------------------
# Ïã§ÌñâÎ∂Ä
# -------------------------------
if __name__ == "__main__":
    crawler = SaraminCrawler()

    # 1) Í≤ÄÏÉâ ‚Üí Í∏∞Î≥∏Ï†ïÎ≥¥ ÏàòÏßë
    df = crawler.crawl_all(sleep_sec=0.6, page_limit=None)
    if df.empty:
        print("Ï¢ÖÎ£å: ÏàòÏßë Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")
        raise SystemExit(0)

    # 2) ÏÉÅÏÑ∏ÌéòÏù¥ÏßÄ Î©ÄÌã∞Ïä§Î†àÎìú ÌååÏã± (ÏõêÎ¨∏ Ï†ïÏ†ú + ÏöîÏïΩ)
    print("üß© ÏÉÅÏÑ∏ÌéòÏù¥ÏßÄ ÌååÏã±(Î©ÄÌã∞Ïä§Î†àÎìú) ÏãúÏûë...")
    df = crawler.enrich_with_details(df, max_workers=8)
    print("üß© ÏÉÅÏÑ∏ÌéòÏù¥ÏßÄ ÌååÏã± ÏôÑÎ£å.")

    # 3) CSV Ï†ÄÏû• (Î∞±ÏóÖ/Í≤ÄÏ¶ùÏö©: Î©îÏùºÏóêÎäî Ï≤®Î∂ÄÌïòÏßÄ ÏïäÏùå)
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_csv = f"saramin_results_raw_{ts}.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"‚úÖ CSV Ï†ÄÏû• ÏôÑÎ£å: {len(df)} rows ‚Üí {out_csv}")

    # 4) HTML Ï†ÄÏû• (GitHub PagesÏö©: ÏõêÎ¨∏+ÏöîÏïΩ Î™®Îëê Ìè¨Ìï®)
    docs_dir = Path("docs")
    html_path = docs_dir / "saramin_results_latest.html"
    pages_url = "https://pkpjs.github.io/test/saramin_results_latest.html"  # ÌïÑÏöîÏãú ÏàòÏ†ï
    crawler.build_html_page(df, str(html_path))

    # 5) Ïù¥Î©îÏùº Î∞úÏÜ° (ÌôòÍ≤ΩÎ≥ÄÏàò ÏÇ¨Ïö©; ÎØ∏ÏÑ§Ï†ï Ïãú Í∏∞Î≥∏ ÏàòÏã†ÏûêÎäî example@gmail.com)
    EMAIL_SENDER   = os.environ.get("EMAIL_SENDER")
    EMAIL_PASSWORD = os.environ.get("EMAIL_APP_PASSWORD")
    EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER", "example@gmail.com")

    if all([EMAIL_SENDER, EMAIL_PASSWORD, EMAIL_RECEIVER]):
        crawler.send_email(
            df=df,
            sender_email=EMAIL_SENDER,
            app_password=EMAIL_PASSWORD,
            receiver_email=EMAIL_RECEIVER,
            pages_url=pages_url
        )
    else:
        print("‚ÑπÔ∏è Ïù¥Î©îÏùº Î∞úÏÜ° ÏÉùÎûµ: EMAIL_SENDER / EMAIL_APP_PASSWORD (Í∑∏Î¶¨Í≥† ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú EMAIL_RECEIVER)Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.")

    print(f"üîó Ï†ÑÏ≤¥ Í≥µÍ≥† ÌéòÏù¥ÏßÄ Ï£ºÏÜå: {pages_url}")
