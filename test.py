import math
import time
import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication

class SaraminCrawler:
    def __init__(self):
        # AJAX ì—”ë“œí¬ì¸íŠ¸
        self.api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"

        # ë¸Œë¼ìš°ì € ìœ„ì¥ í—¤ë”
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.saramin.co.kr/zf_user/search",
            "X-Requested-With": "XMLHttpRequest",
        }

        # ğŸ”§ ê²€ìƒ‰ ì¡°ê±´ (ë‹¹ì‹ ì´ ì¤€ URL ê¸°ì¤€)
        self.params = {
            "searchType": "search",
            "loc_mcd": "106000,104000,105000,107000,110000,111000",
            "cat_kewd": "83,84,85,90,104,108,111,112,114,116",
            "company_cd": "0,1,2,3,4,5,6,7,9,10",
            "exp_cd": "1",
            "exp_none": "y",
            "job_type": "1",
            "search_optional_item": "y",
            "search_done": "y",
            "panel_count": "y",
            "preview": "y",
            "recruitPage": 1,
            "recruitPageCount": 40,
            "recruitSort": "relation"
        }

    def _parse_jobs_from_innerHTML(self, inner_html):
        soup = BeautifulSoup(inner_html, "html.parser")
        jobs = []

        for item in soup.select("div.item_recruit"):
            try:
                rec_idx = item.get("value", "").strip()
                a = item.select_one("h2.job_tit a")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = "https://www.saramin.co.kr" + href if href.startswith("/") else href

                company_el = item.select_one("strong.corp_name a, strong.corp_name")
                company = company_el.get_text(strip=True) if company_el else ""

                cond_spans = item.select("div.job_condition span")
                location = cond_spans[0].get_text(strip=True) if len(cond_spans) > 0 else ""
                career   = cond_spans[1].get_text(strip=True) if len(cond_spans) > 1 else ""
                edu      = cond_spans[2].get_text(strip=True) if len(cond_spans) > 2 else ""

                deadline_el = item.select_one("div.job_date span.date")
                deadline = deadline_el.get_text(strip=True) if deadline_el else ""

                jobs.append({
                    "rec_idx": rec_idx,
                    "title": title,
                    "company": company,
                    "location": location,
                    "career": career,
                    "education": edu,
                    "deadline": deadline,
                    "link": link,
                    "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception:
                continue
        return jobs

    def _fetch_page(self, page: int):
        params = dict(self.params)
        params["recruitPage"] = page
        resp = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        inner_html = data.get("innerHTML", "")
        count_str = data.get("count", "0")
        try:
            total_count = int(str(count_str).replace(",", ""))
        except:
            total_count = 0

        jobs = self._parse_jobs_from_innerHTML(inner_html) if inner_html else []
        return jobs, total_count

    def crawl_all(self, sleep_sec=0.6, page_limit=None):
        print("ğŸ” ìˆ˜ì§‘ ì‹œì‘...")
        all_jobs = []
        first_page_jobs, total_count = self._fetch_page(1)
        if not first_page_jobs:
            print("âš  ì²« í˜ì´ì§€ì—ì„œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í—¤ë”/íŒŒë¼ë¯¸í„° í™•ì¸ í•„ìš”)")
            return pd.DataFrame()

        all_jobs.extend(first_page_jobs)

        page_count = math.ceil(total_count / int(self.params["recruitPageCount"])) if total_count else 1
        if page_limit is not None:
            page_count = min(page_count, page_limit)

        print(f"ğŸ“Š ì´ {total_count}ê±´ ì¶”ì •, {page_count}í˜ì´ì§€ ì˜ˆì •")

        for p in range(2, page_count + 1):
            print(f"ğŸ“„ {p}/{page_count} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            jobs, _ = self._fetch_page(p)
            if not jobs:
                print("â›” ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            all_jobs.extend(jobs)
            time.sleep(sleep_sec)

        df = pd.DataFrame(all_jobs)
        if df.empty:
            print("âš  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        if "rec_idx" in df.columns:
            df["__dedup_key"] = df["rec_idx"].where(df["rec_idx"].astype(bool), other=df["link"])
            df.drop_duplicates(subset=["__dedup_key"], inplace=True)
            df.drop(columns=["__dedup_key"], inplace=True)
        else:
            df.drop_duplicates(subset=["link"], inplace=True)

        return df

    def _get_keyword_stats(self, jobs):
        from collections import Counter
        cnt = Counter([j.get("keyword", "ê¸°íƒ€") for j in jobs])
        return ", ".join([f"{k}({v}ê°œ)" for k, v in cnt.items()]) if cnt else "ì—†ìŒ"

    def send_email(self, jobs, csv_path, sender_email, app_password, receiver_email):
        if not jobs:
            print("âš  ì „ì†¡í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        subject = f"ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼ - {datetime.now().strftime('%Y-%m-%d')}"

        html_body = f"""
        <html>
            <head>
                <meta charset="UTF-8">
                <style>
                    body {{ font-family: 'Apple SD Gothic Neo', Arial, sans-serif; }}
                    .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                            color: white; padding: 20px; text-align: center; }}
                    .job-item {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; 
                            border-radius: 8px; background: #fafafa; }}
                    .job-title {{ font-size: 18px; font-weight: bold; color: #2c3e50; }}
                    .company {{ color: #e74c3c; font-weight: bold; margin: 5px 0; }}
                    .details {{ color: #7f8c8d; font-size: 14px; margin: 5px 0; }}
                    .btn {{ background: #3498db; color: white; padding: 8px 16px; 
                        text-decoration: none; border-radius: 4px; display: inline-block; }}
                    .summary {{ background: #ecf0f1; padding: 15px; margin: 20px 0; border-radius: 8px; }}
                </style>
            </head>
            <body>
                <div class="header">
                    <h1>ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼</h1>
                    <p>{datetime.now().strftime('%Yë…„ %mì›” %dì¼')} ìˆ˜ì§‘ ì™„ë£Œ</p>
                </div>

                <div class="summary">
                    <h2>ğŸ“Š ìˆ˜ì§‘ í˜„í™©</h2>
                    <p>â€¢ <strong>ì´ {len(jobs)}ê°œ</strong> ê³µê³  ë°œê²¬</p>
                    <p>â€¢ í‚¤ì›Œë“œë³„ ë¶„í¬: {self._get_keyword_stats(jobs)}</p>
                    <p>â€¢ ğŸ“ <strong>ì „ì²´ ë°ì´í„°ëŠ” ì²¨ë¶€ëœ CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!</strong></p>
                </div>

                <h2>ğŸ”¥ ì£¼ìš” ê³µê³  ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 10ê°œ)</h2>
        """

        for job in jobs[:10]:
            html_body += f"""
            <div class="job-item">
                <div class="job-title">{job.get('title','')}</div>
                <div class="company">ğŸ¢ {job.get('company','')}</div>
                <div class="details">
                    ğŸ“ {job.get('location','')} | 
                    ğŸ‘” {job.get('career','')} | 
                    ğŸ“ {job.get('education','')} | 
                    â° {job.get('deadline','')}
                </div>
                <a href="{job.get('link','')}" class="btn" target="_blank">ì§€ì›í•˜ê¸° â†’</a>
            </div>
            """

        if len(jobs) > 10:
            html_body += f"""
            <div style="text-align: center; padding: 20px; background: #fff3cd; border-radius: 8px; margin: 20px 0;">
                <h3>ğŸ“‹ ë‚˜ë¨¸ì§€ {len(jobs)-10}ê°œ ê³µê³ </h3>
                <p>ì „ì²´ ê³µê³ ëŠ” <strong>ì²¨ë¶€ëœ CSV íŒŒì¼</strong>ì—ì„œ í™•ì¸í•˜ì„¸ìš”!</p>
            </div>
            """

        html_body += """
                <div style="text-align: center; margin-top: 30px; padding: 20px; background: #f8f9fa;">
                    <p>ğŸ¤– Python ìë™í™” ì‹œìŠ¤í…œì´ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤</p>
                    <p style="font-size: 12px; color: #6c757d;">
                        ë§¤ì¼ ì˜¤ì „ 9ì‹œì— ìƒˆë¡œìš´ ê³µê³ ë¥¼ í™•ì¸í•´ë“œë¦½ë‹ˆë‹¤
                    </p>
                </div>
            </body>
        </html>
        """

        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject

        msg.attach(MIMEText(html_body, 'html', 'utf-8'))

        if csv_path and os.path.exists(csv_path):
            with open(csv_path, 'rb') as f:
                part = MIMEApplication(f.read(), _subtype='csv')
                part.add_header('Content-Disposition', 'attachment',
                                filename=os.path.basename(csv_path))
                msg.attach(part)

        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, app_password)
            server.send_message(msg)
            server.quit()
            print("ğŸ“§ ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    crawler = SaraminCrawler()

    df = crawler.crawl_all(sleep_sec=0.6, page_limit=None)

    if df.empty:
        print("ì¢…ë£Œ: ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
        exit()

    out_csv = f"saramin_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df)} rows â†’ {out_csv}")

    # âœ… GitHub Secretsì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì½ê¸°
    EMAIL_SENDER = os.environ.get("EMAIL_SENDER")
    EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")
    EMAIL_PASSWORD = os.environ.get("EMAIL_APP_PASSWORD")

    if not all([EMAIL_SENDER, EMAIL_RECEIVER, EMAIL_PASSWORD]):
        print("âŒ í™˜ê²½ ë³€ìˆ˜(EMAIL_SENDER / EMAIL_RECEIVER / EMAIL_APP_PASSWORD)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ”” GitHub Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)

    jobs_list = df.to_dict(orient="records")
    crawler.send_email(
        jobs=jobs_list,
        csv_path=out_csv,
        sender_email=EMAIL_SENDER,
        app_password=EMAIL_PASSWORD,
        receiver_email=EMAIL_RECEIVER
    )
