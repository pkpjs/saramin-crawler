import math
import time
import os
import re
import requests
from bs4 import BeautifulSoup, NavigableString
import pandas as pd
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, Optional, List


# -------------------------------
# Saramin Crawler (ê²€ìƒ‰ + ìƒì„¸ íŒŒì‹±: Aëª¨ë“œ=ì›ë¬¸ ì •ë¦¬ + ìš”ì•½)
# -------------------------------
class SaraminCrawler:
    def __init__(self):
        self.api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.saramin.co.kr/zf_user/search",
            "X-Requested-With": "XMLHttpRequest",
        }
        # ğŸ” ê²€ìƒ‰ ì¡°ê±´(ìš”ì²­ ì¡°ê±´)
        self.params = {
            "searchType": "search",
            "loc_mcd": "106000,104000,105000,107000,110000,111000",   # ë¶€ì‚°/ëŒ€êµ¬/ëŒ€ì „/ìš¸ì‚°/ê²½ë‚¨/ê²½ë¶
            "cat_kewd": "83,84,85,90,104,108,111,112,114,116",       # ë°ì´í„°ì—”ì§€ë‹ˆì–´ ì™¸ 10ê°œ
            "company_cd": "0,1,2,3,4,5,6,7,9,10",                    # íšŒì‚¬í˜•íƒœ ì „ì²´
            "exp_cd": "1",                                           # ì‹ ì…
            "exp_none": "y",                                         # ê²½ë ¥ë¬´ê´€ í¬í•¨
            "job_type": "1",                                         # ì •ê·œì§
            "search_optional_item": "y",
            "search_done": "y",
            "panel_count": "y",
            "preview": "y",
            "recruitPage": 1,
            "recruitPageCount": 40,                                  # í˜ì´ì§€ë‹¹ 40ê°œ
            "recruitSort": "relation"                                # ê´€ë ¨ë„ìˆœ
        }

        # ë¶ˆí•„ìš” í…ìŠ¤íŠ¸ ì œê±° íŒ¨í„´(ìƒì„¸ ë³¸ë¬¸ í´ë¦°ì—…)
        self.noise_patterns = [
            r"\bë¡œê·¸ì¸\b", r"\bíšŒì›ê°€ì…\b", r"ê¸°ì—…ì„œë¹„ìŠ¤", r"ì‚¬ëŒì¸\s*ë¹„ì¦ˆë‹ˆìŠ¤", r"ì‚¬ëŒì¸\s*ê³ ê°ì„¼í„°",
            r"\bTOP\b", r"\bì´ì „ê³µê³ \b", r"\bë‹¤ìŒê³µê³ \b", r"ê²€ìƒ‰\s*í¼", r"ê³µì§€ì‚¬í•­", r"ì´ë²¤íŠ¸",
            r"ì»¤ë¦¬ì–´\s*í”¼ë“œ", r"ì‚¬ëŒì¸\s*ìŠ¤í† ì–´", r"ì±„ìš©ì •ë³´\s*ì§€ì—­ë³„", r"HOT100", r"í—¤ë“œí—ŒíŒ…", r"íë ˆì´ì…˜",
            r"íŒŒê²¬ëŒ€í–‰", r"ì™¸êµ­ì¸\s*ì±„ìš©", r"ì¤‘ì¥ë…„\s*ì±„ìš©", r"ì·¨ì—…ì¶•í•˜ê¸ˆ", r"ì‹ ì…Â·ì¸í„´", r"ì±„ìš©ë‹¬ë ¥",
            r"ì—°ë´‰ì •ë³´", r"ë©´ì ‘í›„ê¸°", r"ê¸°ì—…íë ˆì´ì…˜", r"ì´ë ¥ì„œ\s*ì–‘ì‹", r"HRë§¤ê±°ì§„",
            r"ì¸ì ì„±ê²€ì‚¬", r"ì„œë¥˜\s*ì‘ì„±\s*ì½”ì¹­", r"ë©´ì ‘\s*ì½”ì¹­", r"ìê¸°\s*ê³„ë°œ", r"ìê²©ì¦\s*ì¤€ë¹„",
            r"ì‚¬ëŒì¸\s*ì¸ê³µì§€ëŠ¥", r"ë§ì¶¤\s*ê³µê³ ", r"Ai\s*ë§¤ì¹˜", r"Ai\s*ëª¨ì˜ë©´ì ‘"
        ]
        self.noise_regex = re.compile("|".join(self.noise_patterns))

    # ---------- ìœ í‹¸ ----------
    @staticmethod
    def _clean_ws(text: str) -> str:
        if not text:
            return ""
        t = text.replace("\r", "\n")
        t = re.sub(r"\u00A0", " ", t)  # non-breaking space
        t = re.sub(r"[ \t]+", " ", t)
        t = re.sub(r"\n{2,}", "\n", t)
        return t.strip()

    def _is_noise_line(self, line: str) -> bool:
        if not line:
            return True
        if len(line) < 2:
            return True
        if self.noise_regex.search(line):
            return True
        # ì§€ë‚˜ì¹˜ê²Œ ë©”ë‰´ì„± ë‚˜ì—´
        if sum(1 for ch in line if ch == "Â·" or ch == "|") >= 3:
            return True
        return False

    def _dedup_lines_ordered(self, lines: List[str]) -> List[str]:
        seen = set()
        out = []
        for ln in lines:
            k = ln.strip()
            if not k or k in seen:
                continue
            seen.add(k)
            out.append(k)
        return out

    # ---------- ê²€ìƒ‰ê²°ê³¼ íŒŒì‹± ----------
    def _parse_jobs_from_innerHTML(self, inner_html):
        soup = BeautifulSoup(inner_html, "html.parser")
        jobs = []
        for item in soup.select("div.item_recruit"):
            try:
                rec_idx = (item.get("value") or "").strip()
                a = item.select_one("h2.job_tit a")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = "https://www.saramin.co.kr" + href if href.startswith("/") else href

                company_el = item.select_one("strong.corp_name a, strong.corp_name")
                company = company_el.get_text(strip=True) if company_el else ""

                cond_spans = item.select("div.job_condition span")
                location = cond_spans[0].get_text(strip=True) if len(cond_spans) > 0 else ""
                career   = cond_spans[1].get_text(strip=True) if len(cond_spans) > 1 else ""
                edu      = cond_spans[2].get_text(strip=True) if len(cond_spans) > 2 else ""

                deadline_el = item.select_one("div.job_date span.date")
                deadline = deadline_el.get_text(strip=True) if deadline_el else ""

                jobs.append({
                    "rec_idx": rec_idx,
                    "title": title,
                    "company": company,
                    "location": location,
                    "career": career,
                    "education": edu,
                    "deadline": deadline,
                    "link": link,
                    "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception:
                continue
        return jobs

    def _fetch_page(self, page: int):
        params = dict(self.params)
        params["recruitPage"] = page
        resp = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        inner_html = data.get("innerHTML", "")
        count_str = data.get("count", "0")
        try:
            total_count = int(str(count_str).replace(",", ""))
        except Exception:
            total_count = 0

        jobs = self._parse_jobs_from_innerHTML(inner_html) if inner_html else []
        return jobs, total_count

    def crawl_all(self, sleep_sec=0.6, page_limit=None) -> pd.DataFrame:
        print("ğŸ” ìˆ˜ì§‘ ì‹œì‘...")
        all_jobs = []
        first_page_jobs, total_count = self._fetch_page(1)
        if not first_page_jobs:
            print("âš  ì²« í˜ì´ì§€ì—ì„œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í—¤ë”/íŒŒë¼ë¯¸í„° í™•ì¸ í•„ìš”)")
            return pd.DataFrame()

        all_jobs.extend(first_page_jobs)

        page_count = math.ceil(total_count / int(self.params["recruitPageCount"])) if total_count else 1
        if page_limit is not None:
            page_count = min(page_count, page_limit)

        print(f"ğŸ“Š ì´ {total_count}ê±´ ì¶”ì •, {page_count}í˜ì´ì§€ ì˜ˆì •")

        for p in range(2, page_count + 1):
            print(f"ğŸ“„ {p}/{page_count} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            jobs, _ = self._fetch_page(p)
            if not jobs:
                print("â›” ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            all_jobs.extend(jobs)
            time.sleep(sleep_sec)

        df = pd.DataFrame(all_jobs)
        if df.empty:
            print("âš  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        # ì¤‘ë³µ ì œê±°: rec_idx ìš°ì„ , ì—†ìœ¼ë©´ link
        if "rec_idx" in df.columns:
            df["__dedup_key"] = df["rec_idx"].where(df["rec_idx"].astype(bool), other=df["link"])
            df.drop_duplicates(subset=["__dedup_key"], inplace=True)
            df.drop(columns=["__dedup_key"], inplace=True)
        else:
            df.drop_duplicates(subset=["link"], inplace=True)

        return df

    # ---------- ìƒì„¸í˜ì´ì§€ íŒŒì‹± (A ëª¨ë“œ: ì›ë¬¸+ì •ì œ) ----------
    def _find_content_container(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        """
        ìƒì„¸ ë³¸ë¬¸ì´ ë“¤ì–´ìˆëŠ” ìµœìƒìœ„ ì»¨í…Œì´ë„ˆë¥¼ ìµœëŒ€í•œ ì •í™•íˆ ì„ íƒ
        """
        selectors = [
            "#content .wrap_jview",
            "#content .jv-cont",
            "#content .jv_detail",
            "#content .cont",
            "#recruit_info",
            ".wrap_jv_cont",
            ".jview .cont",
            "#content"
        ]
        for sel in selectors:
            node = soup.select_one(sel)
            if node and len(node.get_text(strip=True)) > 50:
                return node
        return soup  # fallback

    def _strip_noise_nodes(self, node: BeautifulSoup):
        """
        ë³¸ë¬¸ ì™¸ ì¡ì˜ì—­ ì œê±°
        """
        for tag in node.find_all(["script", "style", "noscript", "iframe"]):
            tag.decompose()
        for tag in node.find_all(["header", "footer", "nav", "aside"]):
            tag.decompose()
        # ë©”ë‰´/ê´‘ê³ ì„± ì„¹ì…˜ë“¤ ì¶”ì • í´ë˜ìŠ¤ ì œê±°
        junk_classes = [
            "gnb", "lnb", "breadcrumb", "sidebar", "banner", "ad", "advert", "footer",
            "login", "signup", "jv-relate", "sns", "share", "floating", "btn_top"
        ]
        for cls in junk_classes:
            for t in node.select(f".{cls}"):
                t.decompose()

    def _extract_label_value(self, soup: BeautifulSoup, labels: List[str]) -> Optional[str]:
        """
        ë¼ë²¨-ê°’ êµ¬ì¡° ì¶”ì¶œ (dt/dd, th/td, strong/label ë“±)
        """
        nodes = soup.find_all(string=re.compile("|".join([re.escape(x) for x in labels])))
        for node in nodes:
            parent = node.parent
            if not parent:
                continue
            # dt -> dd
            if parent.name == "dt":
                dd = parent.find_next_sibling("dd")
                if dd:
                    return self._clean_ws(dd.get_text(" ", strip=True))
            # th -> td
            if parent.name == "th":
                td = parent.find_next_sibling("td")
                if td:
                    return self._clean_ws(td.get_text(" ", strip=True))
            # strong/label ë°”ë¡œ ë‹¤ìŒ í˜•ì œ
            sib = parent.find_next_sibling()
            if sib and sib.name in ["dd", "td", "p", "div", "span"]:
                val = self._clean_ws(sib.get_text(" ", strip=True))
                if val:
                    return val
            # ê°™ì€ ì¤„ì—ì„œ ì½œë¡  ë“±ìœ¼ë¡œ ì´ì–´ì§„ ì¼€ì´ìŠ¤
            line = self._clean_ws(parent.get_text(" ", strip=True))
            for kw in labels:
                if kw in line:
                    after = line.split(kw, 1)[1].lstrip(": -â€”\t")
                    if after:
                        return self._clean_ws(after)
        return None

    def _extract_section_text(self, node: BeautifulSoup, title_patterns: List[str]) -> Optional[str]:
        """
        ì„¹ì…˜(ìê²©ìš”ê±´/ë³µë¦¬í›„ìƒ ë“±)ì„ ì •ì œ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
        - íƒ€ì´í‹€ê³¼ ê°€ê¹Œìš´ DOMì„ ìš°ì„  ì¶”ì 
        - li/p/div/span í…ìŠ¤íŠ¸ ì·¨í•©
        - ë…¸ì´ì¦ˆ ë¼ì¸ ì œê±°, ì¤‘ë³µ ì œê±°
        """
        regex = re.compile("|".join(title_patterns), re.IGNORECASE)
        hits = node.find_all(string=regex)
        candidates = []

        for hit in hits:
            box = hit
            # ìƒìœ„ë¡œ 2~3ë‹¨ê³„ ì˜¬ë ¤ ì„¹ì…˜ ë˜í¼ ì¶”ì •
            for _ in range(3):
                if box and getattr(box, "parent", None):
                    box = box.parent
            if not box:
                continue

            texts = []
            for t in box.find_all(["li", "p", "dd", "td", "div", "span"]):
                s = t.get_text(" ", strip=True)
                s = self._clean_ws(s)
                if s:
                    texts.append(s)

            if not texts:
                # ì¸ì ‘ í˜•ì œì—ì„œ ì¼ì • ê°œìˆ˜ ì¶”ì¶œ
                sibs = []
                for sib in box.find_all_next(["li", "p", "dd", "td", "div", "span"], limit=60):
                    txt = self._clean_ws(sib.get_text(" ", strip=True))
                    if txt:
                        sibs.append(txt)
                texts = sibs

            # ë…¸ì´ì¦ˆ í•„í„°
            texts = [ln for ln in texts if not self._is_noise_line(ln)]
            # ë„ˆë¬´ ê¸´ ì¤„ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            refined = []
            for ln in texts:
                if len(ln) > 300:
                    parts = re.split(r"[â€¢Â·\-\u2022\|\n]+", ln)
                    for p in parts:
                        p = self._clean_ws(p)
                        if p and not self._is_noise_line(p):
                            refined.append(p)
                else:
                    refined.append(ln)

            refined = self._dedup_lines_ordered(refined)
            if refined:
                candidates.append("\n".join(refined))

        if candidates:
            raw = max(candidates, key=len)  # ê°€ì¥ ê¸´ ê²ƒì„ ì„¹ì…˜ìœ¼ë¡œ ê°„ì£¼
            raw = self._clean_ws(raw[:8000])  # ì•ˆì „ ê¸¸ì´ ì œí•œ
            # ìµœì¢…ì ìœ¼ë¡œ ë©”ë‰´ì„±/ê´‘ê³ ì„± ë¼ì¸ ì¶”ê°€ í•„í„°ë§
            final_lines = [ln for ln in raw.split("\n") if not self._is_noise_line(ln)]
            final = "\n".join(self._dedup_lines_ordered(final_lines))
            return final if final.strip() else None

        return None

    def _summarize_bullets(self, text: str, max_lines: int = 5) -> str:
        """
        ê°„ë‹¨ ê·œì¹™ ê¸°ë°˜ ìš”ì•½:
        - ë¶ˆë¦¿/ìˆ«ì/í‚¤ì›Œë“œ í¬í•¨ ë¼ì¸ ìœ„ì£¼ ì„ ë³„
        - ì „ë¬¸/ê´‘ê³ ì„± ë¬¸êµ¬ ì œê±°
        """
        if not text:
            return ""

        lines = [self._clean_ws(x) for x in text.split("\n")]
        lines = [x for x in lines if x and not self._is_noise_line(x)]

        # í‚¤ì›Œë“œ ì ìˆ˜ ê¸°ë°˜ ì„ ë³„
        keywords = [
            "í•„ìˆ˜", "ìš°ëŒ€", "ê²½ë ¥", "ì‹ ì…", "ê°œë°œ", "ìš´ì˜", "Python", "Java", "C++", "ì„œë²„", "DB",
            "AWS", "OCI", "Linux", "ë„¤íŠ¸ì›Œí¬", "ë³´ì•ˆ", "ìê²©", "í•™ë ¥", "ì „ê³µ", "ìš°ëŒ€ì‚¬í•­", "ë‹´ë‹¹ì—…ë¬´",
            "ê·¼ë¬´", "ê·¼ë¬´ì§€", "ì—°ë´‰", "í˜‘ì˜", "ì •ê·œì§", "ë³µë¦¬í›„ìƒ", "4ëŒ€ë³´í—˜", "ì‹ëŒ€", "ì—°ì°¨", "í‡´ì§ê¸ˆ",
            "í¬íŠ¸í´ë¦¬ì˜¤", "ìê²©ì¦", "ì •ë³´ì²˜ë¦¬", "ì „ë¬¸ì—°", "ë³‘ì—­", "êµ°í•„"
        ]
        kw_re = re.compile("|".join([re.escape(k) for k in keywords]), re.IGNORECASE)

        scored = []
        for ln in lines:
            score = 0
            if re.search(r"^[\-\â€¢\u2022\*\d]+\s", ln):  # ë¶ˆë¦¿/ìˆ«ì ì‹œì‘
                score += 2
            if len(ln) <= 140:
                score += 1
            if kw_re.search(ln):
                score += 3
            scored.append((score, ln))

        scored.sort(key=lambda x: (-x[0], len(x[1])))  # ì ìˆ˜â†“, ê¸¸ì´â†‘
        picked = []
        used = set()
        for _, ln in scored:
            if ln in used:
                continue
            used.add(ln)
            picked.append(ln)
            if len(picked) >= max_lines:
                break

        if not picked:
            # fallback: ì•ë¶€ë¶„ ìƒìœ„ 3~5ì¤„
            picked = lines[:max_lines]

        # ë¶ˆë¦¿ í¬ë§·ìœ¼ë¡œ ì •ë¦¬
        return "â€¢ " + "\nâ€¢ ".join([self._clean_ws(x) for x in picked])

    def _fetch_and_parse_detail(self, session: requests.Session, url: str) -> Tuple[str, Dict[str, str]]:
        """
        ìƒì„¸í˜ì´ì§€ 1ê±´ ìš”ì²­+íŒŒì‹±. (ì„¸ì…˜/íƒ€ì„ì•„ì›ƒ/ë¦¬íŠ¸ë¼ì´ ë‚´ì¥)
        ë°˜í™˜: (url, {employment_type, salary, requirements_raw, benefits_raw, requirements_summary, benefits_summary})
        """
        result = {
            "employment_type": "", "salary": "",
            "requirements_raw": "", "benefits_raw": "",
            "requirements_summary": "", "benefits_summary": ""
        }
        if not url:
            return url, result

        for _ in range(3):
            try:
                resp = session.get(url, timeout=20, headers=self.headers)
                if resp.status_code != 200:
                    time.sleep(0.4)
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")

                content = self._find_content_container(soup)
                self._strip_noise_nodes(content)

                # ë¼ë²¨ ê¸°ë°˜ (ê³ ìš©í˜•íƒœ/ê¸‰ì—¬)
                emp = self._extract_label_value(content, ["ê³ ìš©í˜•íƒœ", "ê·¼ë¬´í˜•íƒœ"])
                sal = self._extract_label_value(content, ["ê¸‰ì—¬", "ì—°ë´‰", "ë³´ìˆ˜", "ê¸‰ì—¬ì¡°ê±´"])

                # ì„¹ì…˜ ê¸°ë°˜ (ìê²©ìš”ê±´/ë³µë¦¬í›„ìƒ)
                req = self._extract_section_text(
                    content,
                    ["ìê²©ìš”ê±´", "ì§€ì›ìê²©", "í•„ìˆ˜ìš”ê±´", "ìš°ëŒ€ì‚¬í•­", "ìš°ëŒ€ì¡°ê±´", "ëª¨ì§‘ìš”ê°•", "ë‹´ë‹¹ì—…ë¬´"]
                )
                ben = self._extract_section_text(
                    content,
                    ["ë³µë¦¬í›„ìƒ", "í˜œíƒ", "ì§€ì›ì œë„", "íšŒì‚¬ë³µì§€"]
                )

                # ìš”ì•½ ìƒì„±
                req_sum = self._summarize_bullets(req or "", max_lines=5) if req else ""
                ben_sum = self._summarize_bullets(ben or "", max_lines=4) if ben else ""

                result.update({
                    "employment_type": emp or "",
                    "salary": sal or "",
                    "requirements_raw": req or "",
                    "benefits_raw": ben or "",
                    "requirements_summary": req_sum,
                    "benefits_summary": ben_sum
                })
                return url, result
            except Exception:
                time.sleep(0.6)
                continue

        return url, result  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’

    def enrich_with_details(self, df: pd.DataFrame, max_workers: int = 8) -> pd.DataFrame:
        """
        ë©€í‹°ìŠ¤ë ˆë“œë¡œ ìƒì„¸í˜ì´ì§€ë¥¼ ë³‘ë ¬ íŒŒì‹±í•˜ì—¬ ì»¬ëŸ¼ ì¶”ê°€ (ì›ë¬¸ + ìš”ì•½)
        """
        if df.empty:
            return df

        urls = df["link"].fillna("").tolist()
        results_map: Dict[str, Dict[str, str]] = {}

        with requests.Session() as session:
            session.headers.update(self.headers)
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = {ex.submit(self._fetch_and_parse_detail, session, url): url for url in urls}
                for fut in as_completed(futures):
                    url, parsed = fut.result()
                    results_map[url] = parsed

        for col in [
            "employment_type", "salary",
            "requirements_raw", "benefits_raw",
            "requirements_summary", "benefits_summary"
        ]:
            df[col] = df["link"].map(lambda u: results_map.get(u, {}).get(col, ""))

        return df

    # ---------- HTML/ì´ë©”ì¼ ----------
    def build_html_page(self, df: pd.DataFrame, out_html_path: str, page_title: str = "ì±„ìš©ê³µê³  ê²°ê³¼(ì •ì œ+ìš”ì•½)"):
        out_path = Path(out_html_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        cols = [
            'title','company','location','career','education','deadline',
            'employment_type','salary',
            'requirements_summary','benefits_summary',
            'requirements_raw','benefits_raw',
            'link','crawled_at'
        ]
        exist_cols = [c for c in cols if c in df.columns]
        styled = df[exist_cols].rename(columns={
            'title':'ì œëª©','company':'íšŒì‚¬','location':'ìœ„ì¹˜','career':'ê²½ë ¥',
            'education':'í•™ë ¥','deadline':'ë§ˆê°ì¼','employment_type':'ê³ ìš©í˜•íƒœ',
            'salary':'ê¸‰ì—¬','requirements_summary':'ìê²©ìš”ê±´(ìš”ì•½)','benefits_summary':'ë³µë¦¬í›„ìƒ(ìš”ì•½)',
            'requirements_raw':'ìê²©ìš”ê±´(ì›ë¬¸)','benefits_raw':'ë³µë¦¬í›„ìƒ(ì›ë¬¸)',
            'link':'ë§í¬','crawled_at':'ìˆ˜ì§‘ì‹œê°'
        }).copy()

        # ë§í¬ ì»¬ëŸ¼ HTMLë¡œ ë³€í™˜
        if 'ë§í¬' in styled.columns:
            styled['ë§í¬'] = styled['ë§í¬'].apply(lambda x: f'<a href="{x}" target="_blank">ë°”ë¡œê°€ê¸°</a>' if x else '')

        # ì›ë¬¸ì€ ì¤„ë°”ê¿ˆ ë³´ì¡´
        for c in ['ìê²©ìš”ê±´(ì›ë¬¸)', 'ë³µë¦¬í›„ìƒ(ì›ë¬¸)', 'ìê²©ìš”ê±´(ìš”ì•½)', 'ë³µë¦¬í›„ìƒ(ìš”ì•½)']:
            if c in styled.columns:
                styled[c] = styled[c].astype(str).str.replace("\n", "<br>")

        table_html = styled.to_html(index=False, escape=False, justify="center", border=0)
        html = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>{page_title}</title>
<style>
body {{ font-family: -apple-system, BlinkMacSystemFont, "Apple SD Gothic Neo", Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }}
.container {{ max-width:1200px; margin:0 auto; }}
h1 {{ font-size:24px; margin-bottom:8px; }}
.desc {{ color:#666; margin-bottom:20px; }}
table {{ width:100%; border-collapse:collapse; table-layout:fixed; word-break:break-word; }}
th, td {{ border-bottom:1px solid #eee; padding:12px 8px; text-align:left; vertical-align:top; }}
th {{ background:#fafafa; font-weight:700; }}
tr:hover {{ background:#f4f9ff; }}
a {{ text-decoration:none; color:#3498db; }}
.meta {{ color:#888; font-size:13px; margin-top:20px; }}
.small {{ color:#888; font-size:12px; }}
</style>
</head>
<body>
<div class="container">
  <h1>{page_title}</h1>
  <div class="desc">ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
  <p class="small">* ë©”ì¼ì—ëŠ” ìš”ì•½ ìœ„ì£¼ë¡œ í‘œì‹œë˜ë©°, ë³¸ í˜ì´ì§€ì—ì„œëŠ” ì›ë¬¸ê³¼ ìš”ì•½ì´ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤.</p>
  {table_html}
  <div class="meta">Powered by Python Â· ìë™ í¬ë¡¤ë§ Â· Updated everyday</div>
</div>
</body>
</html>"""
        out_path.write_text(html, encoding="utf-8")
        print(f"ğŸŒ HTML ìƒì„±: {out_path}")
        return str(out_path)

    def generate_html_table_for_email(self, df: pd.DataFrame, max_rows=10):
        subset = df.head(max_rows).fillna("")
        cols = [
            "title","company","location","employment_type","salary",
            "requirements_summary","benefits_summary"
        ]
        exist = [c for c in cols if c in subset.columns]
        th_map = {
            "title":"ì œëª©","company":"íšŒì‚¬","location":"ìœ„ì¹˜",
            "employment_type":"ê³ ìš©í˜•íƒœ","salary":"ê¸‰ì—¬",
            "requirements_summary":"ìê²©ìš”ê±´(ìš”ì•½)","benefits_summary":"ë³µë¦¬í›„ìƒ(ìš”ì•½)"
        }
        thead = "".join([f"<th>{th_map.get(c,c)}</th>" for c in exist])
        html = "<table style='width:100%; border-collapse:collapse; margin-top:20px;'>"
        html += f"<tr style='background:#667eea; color:white;'>{thead}</tr>"
        for _, row in subset.iterrows():
            html += "<tr>"
            for c in exist:
                val = str(row.get(c, "")).replace("\n", "<br>")
                html += f"<td style='padding:8px 6px; border-bottom:1px solid #eee;'>{val}</td>"
            html += "</tr>"
        html += "</table>"
        return html

    def send_email(self, df: pd.DataFrame, sender_email, app_password, receiver_email, pages_url: str):
        """
        CSV ì²¨ë¶€ ì—†ì´ HTML ë³¸ë¬¸ + ì „ì²´ í˜ì´ì§€ ë§í¬ë§Œ ì „ì†¡
        """
        if df.empty:
            print("âš  ì „ì†¡í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        subject = f"ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼(ì •ì œ+ìš”ì•½) - {datetime.now().strftime('%Y-%m-%d')}"
        html_table = self.generate_html_table_for_email(df, max_rows=10)

        html_body = f"""
        <html><head><meta charset="UTF-8"></head>
        <body style="font-family:'Apple SD Gothic Neo',Arial,sans-serif;">
        <h1>ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼ (ì •ì œ+ìš”ì•½)</h1>
        <p>{datetime.now().strftime('%Yë…„ %mì›” %dì¼')} ìˆ˜ì§‘ ì™„ë£Œ</p>
        <div>
          <h2>ğŸ“Š ìˆ˜ì§‘ í˜„í™©</h2>
          <p>â€¢ <strong>ì´ {len(df)}ê°œ</strong> ê³µê³  ë°œê²¬</p>
        </div>
        <div>
          <h2>ğŸ”¥ ì£¼ìš” ê³µê³  ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 10ê°œ, ìš”ì•½)</h2>
          {html_table}
        </div>
        <div style="text-align:center; margin:30px 0;">
          <a href="{pages_url}" style="display:inline-block; padding:12px 20px; background:#3498db; color:#fff; text-decoration:none; border-radius:6px;">ğŸŒ ì „ì²´ ê³µê³  ë³´ê¸° (ì›ë¬¸+ìš”ì•½)</a>
        </div>
        <p style="font-size:12px; color:#888;">ğŸ¤– CSV íŒŒì¼ì€ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ëŠ” ìƒë‹¨ ë§í¬ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.</p>
        </body></html>
        """

        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(html_body, 'html', 'utf-8'))

        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, app_password)
            server.send_message(msg)
            server.quit()
            print("ğŸ“§ ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ! (ì²¨ë¶€ ì—†ìŒ)")
        except Exception as e:
            print(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")


# -------------------------------
# ì‹¤í–‰ë¶€
# -------------------------------
if __name__ == "__main__":
    crawler = SaraminCrawler()

    # 1) ê²€ìƒ‰ â†’ ê¸°ë³¸ì •ë³´ ìˆ˜ì§‘
    df = crawler.crawl_all(sleep_sec=0.6, page_limit=None)
    if df.empty:
        print("ì¢…ë£Œ: ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
        raise SystemExit(0)

    # 2) ìƒì„¸í˜ì´ì§€ ë©€í‹°ìŠ¤ë ˆë“œ íŒŒì‹± (ì›ë¬¸ ì •ì œ + ìš”ì•½)
    print("ğŸ§© ìƒì„¸í˜ì´ì§€ íŒŒì‹±(ë©€í‹°ìŠ¤ë ˆë“œ) ì‹œì‘...")
    df = crawler.enrich_with_details(df, max_workers=8)
    print("ğŸ§© ìƒì„¸í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ.")

    # 3) CSV ì €ì¥ (ë°±ì—…/ê²€ì¦ìš©: ë©”ì¼ì—ëŠ” ì²¨ë¶€í•˜ì§€ ì•ŠìŒ)
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_csv = f"saramin_results_raw_{ts}.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df)} rows â†’ {out_csv}")

    # 4) HTML ì €ì¥ (GitHub Pagesìš©: ì›ë¬¸+ìš”ì•½ ëª¨ë‘ í¬í•¨)
    docs_dir = Path("docs")
    html_path = docs_dir / "saramin_results_latest.html"
    pages_url = "https://pkpjs.github.io/test/saramin_results_latest.html"  # í•„ìš”ì‹œ ìˆ˜ì •
    crawler.build_html_page(df, str(html_path))

    # 5) ì´ë©”ì¼ ë°œì†¡ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©; ë¯¸ì„¤ì • ì‹œ ê¸°ë³¸ ìˆ˜ì‹ ìëŠ” example@gmail.com)
    EMAIL_SENDER   = os.environ.get("EMAIL_SENDER")
    EMAIL_PASSWORD = os.environ.get("EMAIL_APP_PASSWORD")
    EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER", "example@gmail.com")

    if all([EMAIL_SENDER, EMAIL_PASSWORD, EMAIL_RECEIVER]):
        crawler.send_email(
            df=df,
            sender_email=EMAIL_SENDER,
            app_password=EMAIL_PASSWORD,
            receiver_email=EMAIL_RECEIVER,
            pages_url=pages_url
        )
    else:
        print("â„¹ï¸ ì´ë©”ì¼ ë°œì†¡ ìƒëµ: EMAIL_SENDER / EMAIL_APP_PASSWORD (ê·¸ë¦¬ê³  ì„ íƒì ìœ¼ë¡œ EMAIL_RECEIVER)ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    print(f"ğŸ”— ì „ì²´ ê³µê³  í˜ì´ì§€ ì£¼ì†Œ: {pages_url}")
