import math
import time
import os
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, Optional, List


class SaraminCrawler:
    def __init__(self):
        self.api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.saramin.co.kr/zf_user/search",
            "X-Requested-With": "XMLHttpRequest",
        }
        self.params = {
            "searchType": "search",
            "loc_mcd": "106000,104000,105000,107000,110000,111000",
            "cat_kewd": "83,84,85,90,104,108,111,112,114,116",
            "company_cd": "0,1,2,3,4,5,6,7,9,10",
            "exp_cd": "1",
            "exp_none": "y",
            "job_type": "1",
            "search_optional_item": "y",
            "search_done": "y",
            "panel_count": "y",
            "preview": "y",
            "recruitPage": 1,
            "recruitPageCount": 40,
            "recruitSort": "relation"
        }

    @staticmethod
    def _clean_text(text: str) -> str:
        if not text:
            return ""
        t = text.replace("\r", "\n")
        t = re.sub(r"\u00A0", " ", t)
        t = re.sub(r"[ \t]+", " ", t)
        t = re.sub(r"\n{2,}", "\n", t)
        return t.strip()

    @staticmethod
    def _clean_ws(text: str) -> str:
        if not text:
            return ""
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    @staticmethod
    def _dedup_lines_ordered(lines: List[str]) -> List[str]:
        seen = set()
        out = []
        for ln in lines:
            k = ln.strip()
            if k and k not in seen:
                out.append(k)
                seen.add(k)
        return out

    @staticmethod
    def _is_noise_line(t: str) -> bool:
        if not t:
            return True
        bad = [
            "ë¡œê·¸ì¸", "íšŒì›ê°€ì…", "ê¸°ì—…ì„œë¹„ìŠ¤", "TOP", "ê³ ê°ì„¼í„°", "ì´ë²¤íŠ¸", "ë„ì›€ë§",
            "ì‚¬ëŒì¸", "ì»¤ë¦¬ì–´í”¼ë“œ", "ì¸ì ì„±", "ìŠ¤í† ì–´", "ë©´ì ‘ ì½”ì¹­", "ìì†Œì„œ", "í´ë˜ìŠ¤",
            "ê³µì§€ì‚¬í•­", "ê²€ìƒ‰", "í™ˆ", "ì±„ìš©ì •ë³´", "ê¸°ì—…Â·ì—°ë´‰", "ì»¤ë®¤ë‹ˆí‹°", "SNS", "ê³µìœ ",
            "Copyright", "ì‚¬ì—…ì", "FAX", "help@saramin.co.kr"
        ]
        if len(t) < 3:
            return True
        if any(b in t for b in bad):
            return True
        if re.fullmatch(r"[\-\â€“\â€”\|â€¢Â·]+", t):
            return True
        return False

    def _parse_jobs_from_innerHTML(self, inner_html):
        soup = BeautifulSoup(inner_html, "html.parser")
        jobs = []
        for item in soup.select("div.item_recruit"):
            try:
                rec_idx = (item.get("value") or "").strip()
                a = item.select_one("h2.job_tit a")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = "https://www.saramin.co.kr" + href if href.startswith("/") else href
                company_el = item.select_one("strong.corp_name a, strong.corp_name")
                company = company_el.get_text(strip=True) if company_el else ""
                cond_spans = item.select("div.job_condition span")
                location = cond_spans[0].get_text(strip=True) if len(cond_spans) > 0 else ""
                career   = cond_spans[1].get_text(strip=True) if len(cond_spans) > 1 else ""
                edu      = cond_spans[2].get_text(strip=True) if len(cond_spans) > 2 else ""
                deadline_el = item.select_one("div.job_date span.date")
                deadline = deadline_el.get_text(strip=True) if deadline_el else ""
                jobs.append({
                    "rec_idx": rec_idx,
                    "title": title,
                    "company": company,
                    "location": location,
                    "career": career,
                    "education": edu,
                    "deadline": deadline,
                    "link": link,
                    "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception:
                continue
        return jobs

    def _fetch_page(self, page: int):
        params = dict(self.params)
        params["recruitPage"] = page
        resp = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        inner_html = data.get("innerHTML", "")
        count_str = data.get("count", "0")
        try:
            total_count = int(str(count_str).replace(",", ""))
        except Exception:
            total_count = 0
        jobs = self._parse_jobs_from_innerHTML(inner_html) if inner_html else []
        return jobs, total_count

    def crawl_all(self, sleep_sec=0.6, page_limit=None) -> pd.DataFrame:
        all_jobs = []
        first_page_jobs, total_count = self._fetch_page(1)
        if not first_page_jobs:
            return pd.DataFrame()
        all_jobs.extend(first_page_jobs)
        page_count = math.ceil(total_count / int(self.params["recruitPageCount"])) if total_count else 1
        if page_limit is not None:
            page_count = min(page_count, page_limit)
        for p in range(2, page_count + 1):
            jobs, _ = self._fetch_page(p)
            if not jobs:
                break
            all_jobs.extend(jobs)
            time.sleep(sleep_sec)
        df = pd.DataFrame(all_jobs)
        if df.empty:
            return df
        if "rec_idx" in df.columns:
            df["__dedup_key"] = df["rec_idx"].where(df["rec_idx"].astype(bool), other=df["link"])
            df.drop_duplicates(subset=["__dedup_key"], inplace=True)
            df.drop(columns=["__dedup_key"], inplace=True)
        else:
            df.drop_duplicates(subset=["link"], inplace=True)
        return df

    def _extract_label_value(self, soup: BeautifulSoup, labels: List[str]) -> Optional[str]:
        nodes = soup.find_all(string=re.compile("|".join([re.escape(x) for x in labels])))
        for node in nodes:
            parent = node.parent
            if not parent:
                continue
            if parent.name == "dt":
                dd = parent.find_next_sibling("dd")
                if dd:
                    return self._clean_text(dd.get_text(" ", strip=True))
            if parent.name == "th":
                td = parent.find_next_sibling("td")
                if td:
                    return self._clean_text(td.get_text(" ", strip=True))
            sib = parent.find_next_sibling()
            if sib and sib.name in ["dd", "td", "p", "div", "span"]:
                val = self._clean_text(sib.get_text(" ", strip=True))
                if val:
                    return val
            line = self._clean_text(parent.get_text(" ", strip=True))
            for kw in labels:
                if kw in line:
                    after = line.split(kw, 1)[1].lstrip(": -â€”\t")
                    if after:
                        return self._clean_text(after)
        return None

    def _find_content_container(self, soup: BeautifulSoup) -> Optional[BeautifulSoup]:
        candidates = [
            "#content .wrap_jv_cont",
            "#content .jv_cont",
            ".wrap_jv_cont",
            ".jv_cont",
            "#recruit_info",
            ".content",
            "section[class*=jv]",
            "div[class*=jv_]",
        ]
        best_node = None
        best_length = 0
        for selector in candidates:
            nodes = soup.select(selector)
            for node in nodes:
                text = node.get_text(" ", strip=True)
                if text and len(text) > best_length:
                    best_length = len(text)
                    best_node = node
        if not best_node:
            best_node = soup.select_one("#content") or soup.body or soup
        self._strip_noise_nodes(best_node)
        return best_node

    def _strip_noise_nodes(self, node: BeautifulSoup):
        for tag in node.find_all(["script", "style", "noscript", "header", "footer", "nav", "aside", "iframe"]):
            tag.decompose()
        junk_keywords = [
            "sns", "share", "banner", "ad", "advert", "login", "íšŒì›",
            "ê¸°ì—…ì„œë¹„ìŠ¤", "ê²€ìƒ‰", "TOP", "footer", "ê³ ê°ì„¼í„°", "ì´ë²¤íŠ¸",
            "ì‚¬ëŒì¸ìŠ¤í† ì–´", "ì·¨ì—…TOOL", "í—¤ë“œí—ŒíŒ…", "ì¸ì ì„±ê²€ì‚¬", "ì»¤ë®¤ë‹ˆí‹°",
            "íšŒì‚¬ì†Œê°œ", "ì¸ì¬ì±„ìš©", "íšŒì›ì•½ê´€", "ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨", "ìœ„ì¹˜ê¸°ë°˜",
            "ì œíœ´ë¬¸ì˜", "ë„ì›€ë§", "FAQ", "MY", "ìŠ¤í¬ë©", "ì§€ì›í˜„í™©", "ìµœê·¼ë³¸"
        ]
        for div in node.find_all("div"):
            text = div.get_text(" ", strip=True)
            if any(jk in text for jk in junk_keywords) and len(text) < 300:
                div.decompose()

    def _extract_section(self, content_node: BeautifulSoup, title_keywords: List[str]) -> Optional[str]:
        if not content_node:
            return None
        regex = re.compile("|".join([re.escape(x) for x in title_keywords]), re.IGNORECASE)
        hits = content_node.find_all(string=regex)
        results = []
        for hit in hits:
            section = hit.find_parent()
            for _ in range(3):
                if section and section.parent:
                    section = section.parent
            if not section:
                continue
            texts = []
            for tag in section.find_all(["li", "p", "div", "td", "span"]):
                t = tag.get_text(" ", strip=True)
                if t and not self._is_noise_line(t):
                    texts.append(self._clean_ws(t))
            lines = []
            for text in texts:
                if len(text) > 200:
                    parts = re.split(r"[â€¢Â·\u2022\-\|\n]+", text)
                    for p in parts:
                        p = p.strip()
                        if p and not self._is_noise_line(p):
                            lines.append(p)
                else:
                    lines.append(text)
            lines = self._dedup_lines_ordered(lines)
            if lines and len("\n".join(lines)) > 50:
                results.append("\n".join(lines))
        if results:
            return max(results, key=len)
        return None

    def _summarize(self, text: str, max_lines=5) -> str:
        if not text:
            return ""
        lines = [self._clean_ws(x) for x in text.split("\n") if x.strip()]
        lines = [x for x in lines if not self._is_noise_line(x)]
        keywords = ["ê²½ë ¥", "ì‹ ì…", "ìš°ëŒ€", "í•„ìˆ˜", "ê°œë°œ", "ìš´ì˜", "í•™ë ¥", "ì „ê³µ", "ìê²©ì¦", "í¬íŠ¸í´ë¦¬ì˜¤", "4ëŒ€ë³´í—˜", "ì—°ì°¨", "ë³µì§€", "ê¸‰ì—¬", "ì§€ì›", "ê·¼ë¬´", "ê¸°ìˆ ", "ì–¸ì–´", "ìŠ¤íƒ"]
        scored = []
        for ln in lines:
            score = sum(1 for kw in keywords if kw in ln)
            if re.match(r"^[â€¢\-*\d]+\s*", ln):
                score += 2
            if len(ln) < 150:
                score += 1
            scored.append((score, ln))
        scored.sort(key=lambda x: (-x[0], len(x[1])))
        picked = [ln for _, ln in scored[:max_lines]]
        return "â€¢ " + "\nâ€¢ ".join(picked) if picked else ""

    def _fetch_and_parse_detail(self, session: requests.Session, url: str) -> Tuple[str, Dict[str, str]]:
        result = {"employment_type": "", "salary": "", "requirements_raw": "", "benefits_raw": ""}
        if not url:
            return url, result
        for _ in range(3):
            try:
                resp = session.get(url, timeout=20, headers=self.headers)
                if resp.status_code != 200:
                    time.sleep(0.4)
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")
                content_root = self._find_content_container(soup)
                emp = self._extract_label_value(content_root, ["ê³ ìš©í˜•íƒœ", "ê·¼ë¬´í˜•íƒœ"]) or self._extract_label_value(soup, ["ê³ ìš©í˜•íƒœ", "ê·¼ë¬´í˜•íƒœ"])
                sal = self._extract_label_value(content_root, ["ê¸‰ì—¬", "ì—°ë´‰", "ë³´ìˆ˜", "ê¸‰ì—¬ì¡°ê±´"]) or self._extract_label_value(soup, ["ê¸‰ì—¬", "ì—°ë´‰", "ë³´ìˆ˜", "ê¸‰ì—¬ì¡°ê±´"])
                req = self._extract_section(content_root, ["ìê²©ìš”ê±´", "ì§€ì›ìê²©", "í•„ìˆ˜ìš”ê±´", "ìš°ëŒ€ì‚¬í•­", "ìš°ëŒ€ì¡°ê±´", "ëª¨ì§‘ìš”ê°•", "ë‹´ë‹¹ì—…ë¬´", "ì§ë¬´ë‚´ìš©"])
                ben = self._extract_section(content_root, ["ë³µë¦¬í›„ìƒ", "í˜œíƒ", "ì§€ì›ì œë„", "íšŒì‚¬ë³µì§€", "ê·¼ë¬´í™˜ê²½"])
                result["employment_type"] = emp or ""
                result["salary"] = sal or ""
                result["requirements_raw"] = req or ""
                result["benefits_raw"] = ben or ""
                return url, result
            except Exception:
                time.sleep(0.6)
                continue
        return url, result

    def enrich_with_details(self, df: pd.DataFrame, max_workers: int = 8) -> pd.DataFrame:
        if df.empty:
            return df
        urls = df["link"].fillna("").tolist()
        results_map: Dict[str, Dict[str, str]] = {}
        with requests.Session() as session:
            session.headers.update(self.headers)
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = {ex.submit(self._fetch_and_parse_detail, session, url): url for url in urls}
                for fut in as_completed(futures):
                    url, parsed = fut.result()
                    results_map[url] = parsed
        for col in ["employment_type", "salary", "requirements_raw", "benefits_raw"]:
            df[col] = df["link"].map(lambda u: results_map.get(u, {}).get(col, ""))
        df["requirements_summary"] = df["requirements_raw"].apply(lambda x: self._summarize(x, max_lines=5))
        df["benefits_summary"] = df["benefits_raw"].apply(lambda x: self._summarize(x, max_lines=4))
        return df

    def build_html_page(self, df: pd.DataFrame, out_html_path: str, page_title: str = "ì±„ìš©ê³µê³  ê²°ê³¼(ìš”ì•½)"):
        out_path = Path(out_html_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        cols = [
            'title','company','location','career','education','deadline',
            'employment_type','salary','requirements_summary','benefits_summary',
            'link','crawled_at'
        ]
        exist_cols = [c for c in cols if c in df.columns]
        styled = df[exist_cols].rename(columns={
            'title':'ì œëª©','company':'íšŒì‚¬','location':'ìœ„ì¹˜','career':'ê²½ë ¥',
            'education':'í•™ë ¥','deadline':'ë§ˆê°ì¼','employment_type':'ê³ ìš©í˜•íƒœ',
            'salary':'ê¸‰ì—¬','requirements_summary':'ìê²©ìš”ê±´(ìš”ì•½)','benefits_summary':'ë³µë¦¬í›„ìƒ(ìš”ì•½)',
            'link':'ë§í¬','crawled_at':'ìˆ˜ì§‘ì‹œê°'
        }).copy()
        if 'ë§í¬' in styled.columns:
            styled['ë§í¬'] = styled['ë§í¬'].apply(lambda x: f'<a href="{x}" target="_blank">ë°”ë¡œê°€ê¸°</a>' if x else '')
        table_html = styled.to_html(index=False, escape=False, justify="center", border=0)
        html = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>{page_title}</title>
<style>
body {{ font-family: -apple-system, BlinkMacSystemFont, "Apple SD Gothic Neo", Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }}
.container {{ max-width:1200px; margin:0 auto; }}
h1 {{ font-size:24px; margin-bottom:8px; }}
.desc {{ color:#666; margin-bottom:20px; }}
table {{ width:100%; border-collapse:collapse; table-layout:fixed; word-break:break-word; }}
th, td {{ border-bottom:1px solid #eee; padding:12px 8px; text-align:left; vertical-align:top; }}
th {{ background:#fafafa; font-weight:700; }}
tr:hover {{ background:#f4f9ff; }}
a {{ text-decoration:none; color:#3498db; }}
.meta {{ color:#888; font-size:13px; margin-top:20px; }}
</style>
</head>
<body>
<div class="container">
  <h1>{page_title}</h1>
  <div class="desc">ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
  {table_html}
  <div class="meta">Powered by Python Â· ìë™ í¬ë¡¤ë§ Â· Updated everyday</div>
</div>
</body>
</html>"""
        out_path.write_text(html, encoding="utf-8")
        return str(out_path)

    def generate_html_table_for_email(self, df: pd.DataFrame, max_rows=10):
        subset = df.head(max_rows).fillna("")
        cols = ["title","company","location","employment_type","salary","requirements_summary","benefits_summary"]
        exist = [c for c in cols if c in subset.columns]
        th_map = {
            "title":"ì œëª©","company":"íšŒì‚¬","location":"ìœ„ì¹˜",
            "employment_type":"ê³ ìš©í˜•íƒœ","salary":"ê¸‰ì—¬",
            "requirements_summary":"ìê²©ìš”ê±´(ìš”ì•½)","benefits_summary":"ë³µë¦¬í›„ìƒ(ìš”ì•½)"
        }
        thead = "".join([f"<th>{th_map.get(c,c)}</th>" for c in exist])
        html = "<table style='width:100%; border-collapse:collapse; margin-top:20px;'>"
        html += f"<tr style='background:#667eea; color:white;'>{thead}</tr>"
        for _, row in subset.iterrows():
            html += "<tr>"
            for c in exist:
                val = str(row.get(c, "")).replace("\n", "<br>")
                html += f"<td style='padding:8px 6px; border-bottom:1px solid #eee;'>{val}</td>"
            html += "</tr>"
        html += "</table>"
        return html

    def send_email(self, df: pd.DataFrame, sender_email, app_password, receiver_email, pages_url: str):
        if df.empty:
            print("âš  ì „ì†¡í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        subject = f"ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼(ìš”ì•½) - {datetime.now().strftime('%Y-%m-%d')}"
        html_table = self.generate_html_table_for_email(df, max_rows=10)
        html_body = f"""
        <html><head><meta charset="UTF-8"></head>
        <body style="font-family:'Apple SD Gothic Neo',Arial,sans-serif;">
        <h1>ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼ (ìš”ì•½)</h1>
        <p>{datetime.now().strftime('%Yë…„ %mì›” %dì¼')} ìˆ˜ì§‘ ì™„ë£Œ</p>
        <div>
          <h2>ğŸ“Š ìˆ˜ì§‘ í˜„í™©</h2>
          <p>â€¢ <strong>ì´ {len(df)}ê°œ</strong> ê³µê³  ë°œê²¬</p>
        </div>
        <div>
          <h2>ğŸ”¥ ì£¼ìš” ê³µê³  ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 10ê°œ, ìš”ì•½)</h2>
          {html_table}
        </div>
        <div style="text-align:center; margin:30px 0;">
          <a href="{pages_url}" style="display:inline-block; padding:12px 20px; background:#3498db; color:#fff; text-decoration:none; border-radius:6px;">ğŸŒ ì „ì²´ ê³µê³  ë³´ê¸°</a>
        </div>
        <p style="font-size:12px; color:#888;">ğŸ¤– Python ìë™í™” ì‹œìŠ¤í…œì´ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤</p>
        </body></html>
        """
        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(html_body, 'html', 'utf-8'))
        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, app_password)
            server.send_message(msg)
            server.quit()
            print("ğŸ“§ ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ! (ì²¨ë¶€ ì—†ìŒ)")
        except Exception as e:
            print(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    crawler = SaraminCrawler()
    df = crawler.crawl_all(sleep_sec=0.6, page_limit=None)
    if df.empty:
        print("ì¢…ë£Œ: ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
        raise SystemExit(0)
    print("ğŸ§© ìƒì„¸í˜ì´ì§€ íŒŒì‹±(ë©€í‹°ìŠ¤ë ˆë“œ) ì‹œì‘...")
    df = crawler.enrich_with_details(df, max_workers=8)
    print("ğŸ§© ìƒì„¸í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ.")

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_csv = f"saramin_results_{ts}.csv"
    try:
        df.to_csv(out_csv, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df)} rows â†’ {out_csv}")
    except Exception as e:
        print(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")

    docs_dir = Path("docs")
    html_path = docs_dir / "saramin_results_latest.html"
    pages_url = "https://pkpjs.github.io/test/saramin_results_latest.html"
    crawler.build_html_page(df, str(html_path))

    EMAIL_SENDER   = os.environ.get("EMAIL_SENDER")
    EMAIL_PASSWORD = os.environ.get("EMAIL_APP_PASSWORD")
    EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER", "example@gmail.com")

    if all([EMAIL_SENDER, EMAIL_PASSWORD, EMAIL_RECEIVER]):
        crawler.send_email(
            df=df,
            sender_email=EMAIL_SENDER,
            app_password=EMAIL_PASSWORD,
            receiver_email=EMAIL_RECEIVER,
            pages_url=pages_url
        )
    else:
        print("â„¹ï¸ ì´ë©”ì¼ ë°œì†¡ ìƒëµ: EMAIL_SENDER / EMAIL_APP_PASSWORD (ê·¸ë¦¬ê³  ì„ íƒì ìœ¼ë¡œ EMAIL_RECEIVER)ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    print(f"ğŸ”— ì „ì²´ ê³µê³  í˜ì´ì§€ ì£¼ì†Œ: {pages_url}")
