import math
import time
import os
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, Optional, List, Set

# -------------------------------
# Saramin Crawler (ê²€ìƒ‰ + ìƒì„¸ íŒŒì‹± + ìš”ì•½)
# -------------------------------
class SaraminCrawler:
    def __init__(self):
        self.api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.saramin.co.kr/zf_user/search",
            "X-Requested-With": "XMLHttpRequest",
        }
        # ğŸ” ê²€ìƒ‰ ì¡°ê±´(ìš”ì²­í•˜ì‹  ì¡°ê±´)
        self.params = {
            "searchType": "search",
            "loc_mcd": "106000,104000,105000,107000,110000,111000",   # ë¶€ì‚°/ëŒ€êµ¬/ëŒ€ì „/ìš¸ì‚°/ê²½ë‚¨/ê²½ë¶
            "cat_kewd": "83,84,85,90,104,108,111,112,114,116",       # ë°ì´í„°ì—”ì§€ë‹ˆì–´ ì™¸ 10ê°œ
            "company_cd": "0,1,2,3,4,5,6,7,9,10",                    # íšŒì‚¬í˜•íƒœ ì „ì²´
            "exp_cd": "1",                                           # ì‹ ì…
            "exp_none": "y",                                         # ê²½ë ¥ë¬´ê´€ í¬í•¨
            "job_type": "1",                                         # ì •ê·œì§
            "search_optional_item": "y",
            "search_done": "y",
            "panel_count": "y",
            "preview": "y",
            "recruitPage": 1,
            "recruitPageCount": 40,                                  # í˜ì´ì§€ë‹¹ 40ê°œ
            "recruitSort": "relation"                                # ê´€ë ¨ë„ìˆœ
        }

        # -------- ìš”ì•½ìš© ì‚¬ì „(í‚¤ì›Œë“œ) --------
        # ê¸°ìˆ /ìŠ¤íƒ
        self.skill_keywords = [
            # ì–¸ì–´/í”„ë ˆì„ì›Œí¬
            "python","java","javascript","typescript","go","golang","c","c\\+\\+","c#","scala","ruby","kotlin","swift","php",
            "node","node\\.js","react","react\\.js","vue","vue\\.js","angular","spring","springboot","django","flask",
            ".net","asp\\.net","jpa","hibernate","mybatis",
            # ë°ì´í„°/í”Œë«í¼
            "sql","mysql","postgresql","postgres","mariadb","mssql","oracle","redis","mongodb","elasticsearch","kafka",
            "hadoop","spark","hive","presto","airflow","storm","kinesis","flink","redshift","bigquery","snowflake",
            # ì¸í”„ë¼/í´ë¼ìš°ë“œ/ë„êµ¬
            "aws","gcp","azure","linux","unix","docker","kubernetes","terraform","ansible","nginx","apache","jenkins",
            "git","gitlab","github","ci/cd","cicd","prometheus","grafana","vault",
            # ML/AI/ë³´ì•ˆ
            "pytorch","tensorflow","sklearn","scikit-learn","opencv","nlp","ë¨¸ì‹ ëŸ¬ë‹","ë”¥ëŸ¬ë‹","ai","ml","llm","rag",
            "ë³´ì•ˆ","ì¹¨íˆ¬í…ŒìŠ¤íŠ¸","ëª¨ì˜í•´í‚¹","waf","siem","ids","ips","sso","oauth","saml","kms","kms","kms",
            # ê¸°íƒ€
            "etl","bi","spark streaming","airflow","metabase","tableau","looker","power bi","powerbi","superset"
        ]
        # í•™ë ¥/ìê²©ì¦/ì—°ì°¨/í˜•íƒœ
        self.req_patterns = {
            "degree": [r"ê³ ì¡¸", r"ì´ˆëŒ€ì¡¸", r"ì „ë¬¸í•™ì‚¬", r"í•™ì‚¬", r"ëŒ€ì¡¸", r"ì„ì‚¬", r"ë°•ì‚¬"],
            "years": [r"(\d+)\s*ë…„(?:\s*ì´ìƒ|\s*ì´í•˜|\s*ê²½ë ¥)?", r"ê²½ë ¥\s*(\d+)\s*ë…„"],
            "newbie": [r"ì‹ ì…", r"ê²½ë ¥\s*ë¬´ê´€", r"ë¬´ê´€"],
            "certs": [
                r"ì •ë³´ì²˜ë¦¬ê¸°ì‚¬", r"ì •ë³´ë³´ì•ˆê¸°ì‚¬", r"SQLD", r"ADsP", r"ADP", r"OCP", r"CCNA", r"TOEIC",
                r"AWS[-\s]?SAA", r"AWS[-\s]?SAP", r"AWS[-\s]?DVA", r"GCP[-\s]?PCA", r"CKA", r"CKAD"
            ],
            "must_prefer": [r"í•„ìˆ˜", r"ìš°ëŒ€", r"ìš°ëŒ€ì‚¬í•­", r"ìš°ëŒ€ì¡°ê±´", r"ê°€ì‚°ì ", r"ê°€ì "]
        }
        # ë³µë¦¬í›„ìƒ í‚¤ì›Œë“œ
        self.benefit_keywords = [
            "ì¬íƒ", "ì›ê²©", "í•˜ì´ë¸Œë¦¬ë“œ", "ìœ ì—°ê·¼ë¬´", "íƒ„ë ¥ê·¼ë¬´", "ì‹œì°¨ì¶œê·¼", "ì£¼4ì¼", "ì£¼4.5ì¼",
            "ì—°ë´‰ì¸ìƒ", "ì„±ê³¼ê¸‰", "ì¸ì„¼í‹°ë¸Œ", "ìŠ¤í†¡ì˜µì…˜", "ë³µì§€í¬ì¸íŠ¸", "ëª…ì ˆì„ ë¬¼",
            "ì¤‘ì‹", "ì„ì‹", "ì•¼ê·¼ì‹ëŒ€", "ì‹ëŒ€", "ê°„ì‹", "ì‚¬ë‚´ì¹´í˜",
            "êµí†µë¹„", "í†µì‹ ë¹„", "ì¥ë¹„ì§€ì›", "ìµœì‹ ì¥ë¹„",
            "4ëŒ€ë³´í—˜", "í‡´ì§ê¸ˆ", "í‡´ì§ì—°ê¸ˆ", "ë‹¨ì²´ë³´í—˜", "ìƒí•´ë³´í—˜",
            "ì—°ì°¨", "ë°˜ì°¨", "ë¦¬í”„ë ˆì‹œ", "ì¥ê¸°ê·¼ì†", "íœ´ê°€ë¹„",
            "ê±´ê°•ê²€ì§„", "í—¬ìŠ¤", "ì‚¬ìš°ë‚˜", "ì˜ë£Œë¹„",
            "ìê¸°ê³„ë°œë¹„", "êµìœ¡ë¹„", "ì„¸ë¯¸ë‚˜", "ìŠ¤í„°ë””", "ë„ì„œêµ¬ì…",
            "ê¸°ìˆ™ì‚¬", "ê¸°ìˆ™ì‚¬ì œê³µ", "ì‚¬íƒ", "ì£¼ê±°ì§€ì›",
            "ê²½ì¡°ì‚¬", "ê²½ì¡°ê¸ˆ", "ê²½ì¡°íœ´ê°€", "ì¶œì‚°", "ìœ¡ì•„íœ´ì§", "ëŒë´„",
            "ì‚¬ë‚´ë™ì•„ë¦¬", "ì›Œë¼ë°¸"
        ]

    # ---------- ê²€ìƒ‰ê²°ê³¼ íŒŒì‹± ----------
    def _parse_jobs_from_innerHTML(self, inner_html):
        soup = BeautifulSoup(inner_html, "html.parser")
        jobs = []
        for item in soup.select("div.item_recruit"):
            try:
                rec_idx = (item.get("value") or "").strip()
                a = item.select_one("h2.job_tit a")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = "https://www.saramin.co.kr" + href if href.startswith("/") else href

                company_el = item.select_one("strong.corp_name a, strong.corp_name")
                company = company_el.get_text(strip=True) if company_el else ""

                cond_spans = item.select("div.job_condition span")
                location = cond_spans[0].get_text(strip=True) if len(cond_spans) > 0 else ""
                career   = cond_spans[1].get_text(strip=True) if len(cond_spans) > 1 else ""
                edu      = cond_spans[2].get_text(strip=True) if len(cond_spans) > 2 else ""

                deadline_el = item.select_one("div.job_date span.date")
                deadline = deadline_el.get_text(strip=True) if deadline_el else ""

                jobs.append({
                    "rec_idx": rec_idx,
                    "title": title,
                    "company": company,
                    "location": location,
                    "career": career,
                    "education": edu,
                    "deadline": deadline,
                    "link": link,
                    "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception:
                continue
        return jobs

    def _fetch_page(self, page: int):
        params = dict(self.params)
        params["recruitPage"] = page
        resp = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        inner_html = data.get("innerHTML", "")
        count_str = data.get("count", "0")
        try:
            total_count = int(str(count_str).replace(",", ""))
        except Exception:
            total_count = 0

        jobs = self._parse_jobs_from_innerHTML(inner_html) if inner_html else []
        return jobs, total_count

    def crawl_all(self, sleep_sec=0.6, page_limit=None) -> pd.DataFrame:
        print("ğŸ” ìˆ˜ì§‘ ì‹œì‘...")
        all_jobs = []
        first_page_jobs, total_count = self._fetch_page(1)
        if not first_page_jobs:
            print("âš  ì²« í˜ì´ì§€ì—ì„œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í—¤ë”/íŒŒë¼ë¯¸í„° í™•ì¸ í•„ìš”)")
            return pd.DataFrame()

        all_jobs.extend(first_page_jobs)

        page_count = math.ceil(total_count / int(self.params["recruitPageCount"])) if total_count else 1
        if page_limit is not None:
            page_count = min(page_count, page_limit)

        print(f"ğŸ“Š ì´ {total_count}ê±´ ì¶”ì •, {page_count}í˜ì´ì§€ ì˜ˆì •")

        for p in range(2, page_count + 1):
            print(f"ğŸ“„ {p}/{page_count} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            jobs, _ = self._fetch_page(p)
            if not jobs:
                print("â›” ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            all_jobs.extend(jobs)
            time.sleep(sleep_sec)

        df = pd.DataFrame(all_jobs)
        if df.empty:
            print("âš  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        # ì¤‘ë³µ ì œê±°: rec_idx ìš°ì„ , ì—†ìœ¼ë©´ link
        if "rec_idx" in df.columns:
            df["__dedup_key"] = df["rec_idx"].where(df["rec_idx"].astype(bool), other=df["link"])
            df.drop_duplicates(subset=["__dedup_key"], inplace=True)
            df.drop(columns=["__dedup_key"], inplace=True)
        else:
            df.drop_duplicates(subset=["link"], inplace=True)

        return df

    # ---------- ìƒì„¸í˜ì´ì§€ íŒŒì‹± ----------
    def _extract_label_block(self, soup: BeautifulSoup, label_keywords) -> Optional[str]:
        """
        ìƒì„¸í˜ì´ì§€ì˜ ë‹¤ì–‘í•œ ë§ˆí¬ì—…ì—ì„œ 'ê³ ìš©í˜•íƒœ/ê¸‰ì—¬' ê°™ì€ ë¼ë²¨-ê°’ êµ¬ì¡° ì¶”ì¶œ.
        """
        text_nodes = soup.find_all(string=re.compile("|".join(label_keywords)))
        for node in text_nodes:
            text = node.strip()
            parent = node.parent
            # 1) dt -> dd
            if parent.name == "dt":
                dd = parent.find_next_sibling("dd")
                if dd:
                    return dd.get_text(" ", strip=True)
            # 2) th -> td
            if parent.name == "th":
                td = parent.find_next_sibling("td")
                if td:
                    return td.get_text(" ", strip=True)
            # 3) strong/label ë“± -> ë‹¤ìŒ í˜•ì œ
            sib = parent.find_next_sibling()
            if sib and sib.name in ["dd", "td", "p", "div", "span"]:
                val = sib.get_text(" ", strip=True)
                if val:
                    return val
            # 4) ê°™ì€ ì¤„ ì½œë¡ /ìŠ¤í˜ì´ìŠ¤ ë¶„ë¦¬
            line = parent.get_text(" ", strip=True)
            for kw in label_keywords:
                if kw in line:
                    after = line.split(kw, 1)[1].strip(" :-â€”\t")
                    if after:
                        return after
        return None

    def _extract_long_section(self, soup: BeautifulSoup, keywords_regex: str) -> Optional[str]:
        """
        ìê²©ìš”ê±´ / ë³µë¦¬í›„ìƒ ë“± ê¸´ í…ìŠ¤íŠ¸ ì„¹ì…˜ì„ ìµœëŒ€í•œ ë„“ê²Œ ê¸ì–´ì™€ ì›ë¬¸ ë°˜í™˜.
        """
        candidates = []
        section_nodes = soup.find_all(string=re.compile(keywords_regex))
        for node in section_nodes:
            box = node
            # ìƒìœ„ë¡œ ì˜¬ë ¤ ì„¹ì…˜ ì»¨í…Œì´ë„ˆ ì¶”ì •
            for _ in range(2):
                if box and box.parent:
                    box = box.parent
            if not box:
                continue
            texts = [t.get_text(" ", strip=True)
                     for t in box.find_all(["li", "p", "dd", "td", "div", "span"])
                     if t.get_text(strip=True)]
            if not texts:
                sibs = []
                for sib in box.find_all_next(["li", "p", "dd", "td", "div", "span"], limit=30):
                    txt = sib.get_text(" ", strip=True)
                    if txt:
                        sibs.append(txt)
                texts = sibs
            chunk = "\n".join(texts)
            if chunk:
                candidates.append(chunk)
        if candidates:
            return max(candidates, key=len)[:6000]
        return None

    def _fetch_and_parse_detail(self, session: requests.Session, url: str) -> Tuple[str, Dict[str, str]]:
        """
        ìƒì„¸í˜ì´ì§€ 1ê±´ ìš”ì²­+íŒŒì‹±. (ì„¸ì…˜/íƒ€ì„ì•„ì›ƒ/ë¦¬íŠ¸ë¼ì´ ë‚´ì¥)
        ë°˜í™˜: (url, {employment_type, salary, requirements_raw, benefits_raw})
        """
        result = {"employment_type": "", "salary": "", "requirements_raw": "", "benefits_raw": ""}
        if not url:
            return url, result

        for _ in range(3):
            try:
                resp = session.get(url, timeout=20, headers=self.headers)
                if resp.status_code != 200:
                    time.sleep(0.4)
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")

                emp = self._extract_label_block(soup, ["ê³ ìš©í˜•íƒœ"])
                sal = self._extract_label_block(soup, ["ê¸‰ì—¬", "ì—°ë´‰", "ë³´ìˆ˜"])

                req = self._extract_long_section(soup, r"(ìê²©ìš”ê±´|í•„ìˆ˜ìš”ê±´|ìš°ëŒ€ì‚¬í•­|ìš°ëŒ€ì¡°ê±´)")
                ben = self._extract_long_section(soup, r"(ë³µë¦¬í›„ìƒ|í˜œíƒ|ì§€ì›ì œë„)")

                result["employment_type"] = emp or ""
                result["salary"]          = sal or ""
                result["requirements_raw"] = req or ""
                result["benefits_raw"]     = ben or ""
                return url, result
            except Exception:
                time.sleep(0.6)
                continue

        return url, result  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’

    def enrich_with_details(self, df: pd.DataFrame, max_workers: int = 8) -> pd.DataFrame:
        """
        ë©€í‹°ìŠ¤ë ˆë“œë¡œ ìƒì„¸í˜ì´ì§€ë¥¼ ë³‘ë ¬ íŒŒì‹±í•˜ì—¬ ì»¬ëŸ¼ ì¶”ê°€
        """
        if df.empty:
            return df

        urls = df["link"].fillna("").tolist()
        results_map = {}

        with requests.Session() as session:
            session.headers.update(self.headers)
            with ThreadPoolExecutor(max_workers=max_workers) as ex:
                futures = {ex.submit(self._fetch_and_parse_detail, session, url): url for url in urls}
                for fut in as_completed(futures):
                    url, parsed = fut.result()
                    results_map[url] = parsed

        for col in ["employment_type", "salary", "requirements_raw", "benefits_raw"]:
            df[col] = df["link"].map(lambda u: results_map.get(u, {}).get(col, ""))

        return df

    # ---------- ìš”ì•½(í•µì‹¬ í‚¤ì›Œë“œ ì „ë¶€ ì¶”ì¶œ) ----------
    @staticmethod
    def _normalize_text(t: str) -> str:
        t = re.sub(r"[ \t]+", " ", t)
        t = t.replace("\r", "\n")
        t = re.sub(r"\n{2,}", "\n", t)
        return t.strip()

    def _extract_keywords(self, text: str, patterns: Dict[str, List[str]]) -> Set[str]:
        """íŒ¨í„´ dictë¡œ ì¼ì¹˜í•˜ëŠ” í•­ëª© ì „ë¶€ ì¶”ì¶œ"""
        hits: Set[str] = set()
        for _, regs in patterns.items():
            for rgx in regs:
                for m in re.findall(rgx, text, flags=re.IGNORECASE):
                    if isinstance(m, tuple):
                        m = " ".join([x for x in m if x])
                    hits.add(str(m).strip())
        return hits

    def _extract_skills(self, text: str) -> Set[str]:
        """ìŠ¤í‚¬ í‚¤ì›Œë“œ ì „ë¶€ ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì/êµ¬ë‘ì  ë¬´ì‹œ)"""
        t = text.lower()
        # êµ¬ë¶„ì ì •ë¦¬
        t = re.sub(r"[,/|Â·â€¢ãƒ»â€¢\-\(\)\[\]{}]", " ", t)
        found: Set[str] = set()
        for kw in self.skill_keywords:
            if re.search(rf"(?<![a-z0-9+]){kw}(?![a-z0-9+])", t, flags=re.IGNORECASE):
                # í‘œê¸° í†µì¼
                norm = kw.replace("\\+\\+", "C++").replace("\\.", ".")
                norm = norm.upper() if norm in ["aws","gcp","sql","ci/cd","cicd","nlp","ai","ml"] else norm
                # ë³´ê¸° ì¢‹ê²Œ ëŒ€ë¬¸ìí™”/íƒ€ì´í‹€í™”
                if norm.lower() in ["python","java","javascript","typescript","golang","node","react","vue","angular","django","flask","spring","springboot"]:
                    norm = norm.title().replace("Javascript","JavaScript").replace("Typescript","TypeScript")
                found.add(norm)
        return found

    def summarize_requirements(self, raw: str) -> str:
        if not raw:
            return ""
        text = self._normalize_text(raw)
        skills  = self._extract_skills(text)
        extras  = self._extract_keywords(text, self.req_patterns)

        # 'í•„ìˆ˜/ìš°ëŒ€' ë¬¸êµ¬ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
        # ì—°ì°¨/í•™ë ¥/ìê²©ì¦/ì‹ ì…ì—¬ë¶€ ë“±ì€ extrasì— ë“¤ì–´ìˆìŒ
        # ë³´ê¸°ì¢‹ê²Œ ì •ë ¬
        out = sorted(skills | extras, key=lambda s: s.lower())
        return " / ".join(out)

    def summarize_benefits(self, raw: str) -> str:
        if not raw:
            return ""
        text = self._normalize_text(raw)
        # í‚¤ì›Œë“œ ì „ë¶€ í¬í•¨
        hits: Set[str] = set()
        for kw in self.benefit_keywords:
            if re.search(kw, text, flags=re.IGNORECASE):
                hits.add(kw)
        out = sorted(hits, key=lambda s: s.lower())
        return " / ".join(out)

    def add_summaries(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            return df
        df["requirements_summary"] = df["requirements_raw"].apply(self.summarize_requirements)
        df["benefits_summary"]     = df["benefits_raw"].apply(self.summarize_benefits)
        return df

    # ---------- HTML/ì´ë©”ì¼ ----------
    def build_html_page(self, df: pd.DataFrame, out_html_path: str, page_title: str = "ì±„ìš©ê³µê³  ê²°ê³¼(ìš”ì•½í˜•)"):
        out_path = Path(out_html_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        cols = [
            'title','company','location','career','education','deadline',
            'employment_type','salary','requirements_summary','benefits_summary',
            'link','crawled_at'
        ]
        exist_cols = [c for c in cols if c in df.columns]
        styled = df[exist_cols].rename(columns={
            'title':'ì œëª©','company':'íšŒì‚¬','location':'ìœ„ì¹˜','career':'ê²½ë ¥',
            'education':'í•™ë ¥','deadline':'ë§ˆê°ì¼','employment_type':'ê³ ìš©í˜•íƒœ',
            'salary':'ê¸‰ì—¬','requirements_summary':'ìê²©ìš”ê±´(ìš”ì•½)','benefits_summary':'ë³µë¦¬í›„ìƒ(ìš”ì•½)',
            'link':'ë§í¬','crawled_at':'ìˆ˜ì§‘ì‹œê°'
        }).copy()

        # ë§í¬ ì»¬ëŸ¼ HTMLë¡œ ë³€í™˜
        if 'ë§í¬' in styled.columns:
            styled['ë§í¬'] = styled['ë§í¬'].apply(lambda x: f'<a href="{x}" target="_blank">ë°”ë¡œê°€ê¸°</a>' if x else '')

        table_html = styled.to_html(index=False, escape=False, justify="center", border=0)
        html = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>{page_title}</title>
<style>
body {{ font-family: -apple-system, BlinkMacSystemFont, "Apple SD Gothic Neo", Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }}
.container {{ max-width:1200px; margin:0 auto; }}
h1 {{ font-size:24px; margin-bottom:8px; }}
.desc {{ color:#666; margin-bottom:20px; }}
table {{ width:100%; border-collapse:collapse; }}
th, td {{ border-bottom:1px solid #eee; padding:12px 8px; text-align:left; vertical-align:top; }}
th {{ background:#fafafa; font-weight:700; }}
tr:hover {{ background:#f4f9ff; }}
a {{ text-decoration:none; color:#3498db; }}
.meta {{ color:#888; font-size:13px; margin-top:20px; }}
</style>
</head>
<body>
<div class="container">
  <h1>{page_title}</h1>
  <div class="desc">ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
  {table_html}
  <div class="meta">Powered by Python Â· ìë™ í¬ë¡¤ë§ Â· Updated everyday</div>
</div>
</body>
</html>"""
        out_path.write_text(html, encoding="utf-8")
        print(f"ğŸŒ HTML ìƒì„±: {out_path}")
        return str(out_path)

    def generate_html_table_for_email(self, df: pd.DataFrame, max_rows=10):
        subset = df.head(max_rows).fillna("")
        cols = ["title","company","location","employment_type","salary","requirements_summary","benefits_summary"]
        exist = [c for c in cols if c in subset.columns]
        th_map = {
            "title":"ì œëª©","company":"íšŒì‚¬","location":"ìœ„ì¹˜",
            "employment_type":"ê³ ìš©í˜•íƒœ","salary":"ê¸‰ì—¬",
            "requirements_summary":"ìê²©ìš”ê±´(ìš”ì•½)","benefits_summary":"ë³µë¦¬í›„ìƒ(ìš”ì•½)"
        }
        thead = "".join([f"<th>{th_map.get(c,c)}</th>" for c in exist])
        html = "<table style='width:100%; border-collapse:collapse; margin-top:20px;'>"
        html += f"<tr style='background:#667eea; color:white;'>{thead}</tr>"
        for _, row in subset.iterrows():
            html += "<tr>"
            for c in exist:
                val = str(row.get(c, ""))
                html += f"<td style='padding:8px 6px; border-bottom:1px solid #eee;'>{val}</td>"
            html += "</tr>"
        html += "</table>"
        return html

    def send_email(self, df: pd.DataFrame, csv_path, sender_email, app_password, receiver_email, pages_url: str):
        if df.empty:
            print("âš  ì „ì†¡í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        subject = f"ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼(ìš”ì•½í˜•) - {datetime.now().strftime('%Y-%m-%d')}"
        html_table = self.generate_html_table_for_email(df, max_rows=10)

        html_body = f"""
        <html><head><meta charset="UTF-8"></head>
        <body style="font-family:'Apple SD Gothic Neo',Arial,sans-serif;">
        <h1>ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼ (ìš”ì•½í˜•)</h1>
        <p>{datetime.now().strftime('%Yë…„ %mì›” %dì¼')} ìˆ˜ì§‘ ì™„ë£Œ</p>
        <div>
          <h2>ğŸ“Š ìˆ˜ì§‘ í˜„í™©</h2>
          <p>â€¢ <strong>ì´ {len(df)}ê°œ</strong> ê³µê³  ë°œê²¬</p>
        </div>
        <div>
          <h2>ğŸ”¥ ì£¼ìš” ê³µê³  ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 10ê°œ)</h2>
          {html_table}
        </div>
        <div style="text-align:center; margin:30px 0;">
          <a href="{pages_url}" style="display:inline-block; padding:12px 20px; background:#3498db; color:#fff; text-decoration:none; border-radius:6px;">ğŸŒ ì „ì²´ ê³µê³  ë³´ê¸°</a>
        </div>
        <p style="font-size:12px; color:#888;">ğŸ¤– Python ìë™í™” ì‹œìŠ¤í…œì´ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤</p>
        </body></html>
        """

        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(html_body, 'html', 'utf-8'))

        # CSV ì²¨ë¶€
        if csv_path and os.path.exists(csv_path):
            with open(csv_path, 'rb') as f:
                part = MIMEApplication(f.read(), _subtype='csv')
                part.add_header('Content-Disposition', 'attachment', filename=os.path.basename(csv_path))
                msg.attach(part)

        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, app_password)
            server.send_message(msg)
            server.quit()
            print("ğŸ“§ ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")


# -------------------------------
# ì‹¤í–‰ë¶€
# -------------------------------
if __name__ == "__main__":
    crawler = SaraminCrawler()

    # 1) ê²€ìƒ‰ â†’ ê¸°ë³¸ì •ë³´ ìˆ˜ì§‘
    df = crawler.crawl_all(sleep_sec=0.6, page_limit=None)
    if df.empty:
        print("ì¢…ë£Œ: ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
        raise SystemExit(0)

    # 2) ìƒì„¸í˜ì´ì§€ ë©€í‹°ìŠ¤ë ˆë“œ íŒŒì‹± (ì›ë¬¸ ìˆ˜ì§‘)
    print("ğŸ§© ìƒì„¸í˜ì´ì§€ íŒŒì‹±(ë©€í‹°ìŠ¤ë ˆë“œ) ì‹œì‘...")
    df = crawler.enrich_with_details(df, max_workers=8)
    print("ğŸ§© ìƒì„¸í˜ì´ì§€ íŒŒì‹± ì™„ë£Œ.")

    # 3) ìš”ì•½ ì»¬ëŸ¼ ìƒì„± (í•µì‹¬ í‚¤ì›Œë“œ ì „ë¶€)
    df = crawler.add_summaries(df)

    # 4) CSV ì €ì¥
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_csv = f"saramin_results_summary_{ts}.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df)} rows â†’ {out_csv}")

    # 5) HTML ì €ì¥ (GitHub Pagesìš©)
    docs_dir = Path("docs")
    html_path = docs_dir / "saramin_results_latest.html"
    pages_url = "https://pkpjs.github.io/test/saramin_results_latest.html"  # í•„ìš”ì‹œ ìˆ˜ì •
    crawler.build_html_page(df, str(html_path))

    # 6) ì´ë©”ì¼ ë°œì†¡ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©; ë¯¸ì„¤ì • ì‹œ ê¸°ë³¸ ìˆ˜ì‹ ìëŠ” example@gmail.com)
    EMAIL_SENDER   = os.environ.get("EMAIL_SENDER")
    EMAIL_PASSWORD = os.environ.get("EMAIL_APP_PASSWORD")
    EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER", "example@gmail.com")

    if all([EMAIL_SENDER, EMAIL_PASSWORD, EMAIL_RECEIVER]):
        crawler.send_email(
            df=df,
            csv_path=out_csv,
            sender_email=EMAIL_SENDER,
            app_password=EMAIL_PASSWORD,
            receiver_email=EMAIL_RECEIVER,
            pages_url=pages_url
        )
    else:
        print("â„¹ï¸ ì´ë©”ì¼ ë°œì†¡ ìƒëµ: EMAIL_SENDER / EMAIL_APP_PASSWORD (ê·¸ë¦¬ê³  ì„ íƒì ìœ¼ë¡œ EMAIL_RECEIVER)ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    print(f"ğŸ”— ì „ì²´ ê³µê³  í˜ì´ì§€ ì£¼ì†Œ: {pages_url}")
