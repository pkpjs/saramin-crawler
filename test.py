import math
import time
import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from pathlib import Path

class SaraminCrawler:
    def __init__(self):
        self.api_url = "https://www.saramin.co.kr/zf_user/search/get-recruit-list"
        self.headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/124.0.0.0 Safari/537.36"
            ),
            "Referer": "https://www.saramin.co.kr/zf_user/search",
            "X-Requested-With": "XMLHttpRequest",
        }
        # ğŸ” ê²€ìƒ‰ ì¡°ê±´(ìš”ì²­ ì¡°ê±´)
        self.params = {
            "searchType": "search",
            "loc_mcd": "106000,104000,105000,107000,110000,111000",   # ë¶€ì‚°/ëŒ€êµ¬/ëŒ€ì „/ìš¸ì‚°/ê²½ë‚¨/ê²½ë¶
            "cat_kewd": "83,84,85,90,104,108,111,112,114,116",       # ë°ì´í„°ì—”ì§€ë‹ˆì–´ ì™¸ 10ê°œ
            "company_cd": "0,1,2,3,4,5,6,7,9,10",                    # íšŒì‚¬í˜•íƒœ ì „ì²´
            "exp_cd": "1",                                           # ì‹ ì…
            "exp_none": "y",                                         # ê²½ë ¥ë¬´ê´€ í¬í•¨
            "job_type": "1",                                         # ì •ê·œì§
            "search_optional_item": "y",
            "search_done": "y",
            "panel_count": "y",
            "preview": "y",
            "recruitPage": 1,
            "recruitPageCount": 40,                                  # í˜ì´ì§€ë‹¹ 40ê°œ
            "recruitSort": "relation"                                # ê´€ë ¨ë„ìˆœ
        }

    # ---------- ê²€ìƒ‰ê²°ê³¼ íŒŒì‹± ----------
    def _parse_jobs_from_innerHTML(self, inner_html):
        soup = BeautifulSoup(inner_html, "html.parser")
        jobs = []
        for item in soup.select("div.item_recruit"):
            try:
                rec_idx = (item.get("value") or "").strip()
                a = item.select_one("h2.job_tit a")
                if not a:
                    continue
                title = a.get_text(strip=True)
                href = a.get("href", "")
                link = "https://www.saramin.co.kr" + href if href.startswith("/") else href

                company_el = item.select_one("strong.corp_name a, strong.corp_name")
                company = company_el.get_text(strip=True) if company_el else ""

                cond_spans = item.select("div.job_condition span")
                location = cond_spans[0].get_text(strip=True) if len(cond_spans) > 0 else ""
                career   = cond_spans[1].get_text(strip=True) if len(cond_spans) > 1 else ""
                edu      = cond_spans[2].get_text(strip=True) if len(cond_spans) > 2 else ""

                deadline_el = item.select_one("div.job_date span.date")
                deadline = deadline_el.get_text(strip=True) if deadline_el else ""

                jobs.append({
                    "rec_idx": rec_idx,
                    "title": title,
                    "company": company,
                    "location": location,
                    "career": career,
                    "education": edu,
                    "deadline": deadline,
                    "link": link,
                    "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
            except Exception:
                continue
        return jobs

    def _fetch_page(self, page: int):
        params = dict(self.params)
        params["recruitPage"] = page
        resp = requests.get(self.api_url, params=params, headers=self.headers, timeout=30)
        resp.raise_for_status()
        data = resp.json()

        inner_html = data.get("innerHTML", "")
        count_str = data.get("count", "0")
        try:
            total_count = int(str(count_str).replace(",", ""))
        except Exception:
            total_count = 0

        jobs = self._parse_jobs_from_innerHTML(inner_html) if inner_html else []
        return jobs, total_count

    def crawl_all(self, sleep_sec=0.6, page_limit=None) -> pd.DataFrame:
        print("ğŸ” ìˆ˜ì§‘ ì‹œì‘...")
        all_jobs = []
        first_page_jobs, total_count = self._fetch_page(1)
        if not first_page_jobs:
            print("âš  ì²« í˜ì´ì§€ì—ì„œ ê³µê³ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í—¤ë”/íŒŒë¼ë¯¸í„° í™•ì¸ í•„ìš”)")
            return pd.DataFrame()

        all_jobs.extend(first_page_jobs)

        page_count = math.ceil(total_count / int(self.params["recruitPageCount"])) if total_count else 1
        if page_limit is not None:
            page_count = min(page_count, page_limit)

        print(f"ğŸ“Š ì´ {total_count}ê±´ ì¶”ì •, {page_count}í˜ì´ì§€ ì˜ˆì •")

        for p in range(2, page_count + 1):
            print(f"ğŸ“„ {p}/{page_count} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
            jobs, _ = self._fetch_page(p)
            if not jobs:
                print("â›” ë” ì´ìƒ ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                break
            all_jobs.extend(jobs)
            time.sleep(sleep_sec)

        df = pd.DataFrame(all_jobs)
        if df.empty:
            print("âš  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df

        # ì¤‘ë³µ ì œê±°: rec_idx ìš°ì„ , ì—†ìœ¼ë©´ link
        if "rec_idx" in df.columns:
            df["__dedup_key"] = df["rec_idx"].where(df["rec_idx"].astype(bool), other=df["link"])
            df.drop_duplicates(subset=["__dedup_key"], inplace=True)
            df.drop(columns=["__dedup_key"], inplace=True)
        else:
            df.drop_duplicates(subset=["link"], inplace=True)

        return df

    # ---------- HTML/ì´ë©”ì¼ ----------
    def build_html_page(self, df: pd.DataFrame, out_html_path: str, page_title: str = "ì±„ìš©ê³µê³  ê²°ê³¼"):
        """DataFrameì„ HTML í˜ì´ì§€ë¡œ ì €ì¥ (GitHub Pages ìš©)"""
        out_path = Path(out_html_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)

        styled = (
            df[['title','company','location','career','education','deadline','link','crawled_at']]
            .rename(columns={
                'title':'ì œëª©','company':'íšŒì‚¬','location':'ìœ„ì¹˜',
                'career':'ê²½ë ¥','education':'í•™ë ¥','deadline':'ë§ˆê°ì¼',
                'link':'ë§í¬','crawled_at':'ìˆ˜ì§‘ì‹œê°'
            })
            .copy()
        )

        # ë§í¬ ì»¬ëŸ¼ HTMLë¡œ ë³€í™˜ (ìƒˆ ì°½)
        styled['ë§í¬'] = styled['ë§í¬'].apply(lambda x: f'<a href="{x}" target="_blank" rel="noopener">ë°”ë¡œê°€ê¸°</a>' if x else '')

        table_html = styled.to_html(index=False, escape=False, justify="center", border=0)

        html = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>{page_title}</title>
<style>
body {{ font-family: -apple-system, BlinkMacSystemFont, "Apple SD Gothic Neo", Segoe UI, Roboto, Arial, sans-serif; margin: 24px; }}
.container {{ max-width:1200px; margin:0 auto; }}
h1 {{ font-size:24px; margin-bottom:8px; }}
.desc {{ color:#666; margin-bottom:20px; }}
table {{ width:100%; border-collapse:collapse; table-layout:fixed; word-break:break-word; }}
th, td {{ border-bottom:1px solid #eee; padding:12px 8px; text-align:left; vertical-align:top; }}
th {{ background:#fafafa; font-weight:700; }}
tr:hover {{ background:#f4f9ff; }}
a {{ text-decoration:none; color:#3498db; }}
.meta {{ color:#888; font-size:13px; margin-top:20px; }}
</style>
</head>
<body>
<div class="container">
  <h1>{page_title}</h1>
  <div class="desc">ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
  {table_html}
  <div class="meta">Powered by Python Â· ìë™ í¬ë¡¤ë§ Â· Updated everyday</div>
</div>
</body>
</html>"""
        out_path.write_text(html, encoding="utf-8")
        print(f"ğŸŒ HTML ìƒì„±: {out_path}")
        return str(out_path)

    def generate_html_table_for_email(self, jobs, max_rows=10):
        """ì´ë©”ì¼ ë³¸ë¬¸ìš© í…Œì´ë¸” (ìƒìœ„ max_rowsê°œ)"""
        subset = jobs[:max_rows]
        html = "<table style='width:100%; border-collapse:collapse; margin-top:20px;'>"
        html += "<tr style='background:#667eea; color:white;'><th>ì œëª©</th><th>íšŒì‚¬</th><th>ìœ„ì¹˜</th><th>ê²½ë ¥</th><th>í•™ë ¥</th><th>ë§ˆê°ì¼</th></tr>"
        for job in subset:
            html += (
                f"<tr>"
                f"<td>{job.get('title','')}</td>"
                f"<td>{job.get('company','')}</td>"
                f"<td>{job.get('location','')}</td>"
                f"<td>{job.get('career','')}</td>"
                f"<td>{job.get('education','')}</td>"
                f"<td>{job.get('deadline','')}</td>"
                f"</tr>"
            )
        html += "</table>"
        return html

    def send_email(self, jobs, sender_email, app_password, receiver_email, pages_url: str):
        """CSV ì²¨ë¶€ ì—†ì´ HTML ë³¸ë¬¸ë§Œ ì „ì†¡"""
        if not jobs:
            print("âš  ì „ì†¡í•  ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        subject = f"ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼ - {datetime.now().strftime('%Y-%m-%d')}"
        html_table = self.generate_html_table_for_email(jobs, max_rows=10)

        html_body = f"""
        <html><head><meta charset="UTF-8"></head>
        <body style="font-family:'Apple SD Gothic Neo',Arial,sans-serif;">
        <h1>ğŸ¯ ì±„ìš©ê³µê³  ìë™ ìˆ˜ì§‘ ê²°ê³¼</h1>
        <p>{datetime.now().strftime('%Yë…„ %mì›” %dì¼')} ìˆ˜ì§‘ ì™„ë£Œ</p>
        <div>
          <h2>ğŸ“Š ìˆ˜ì§‘ í˜„í™©</h2>
          <p>â€¢ <strong>ì´ {len(jobs)}ê°œ</strong> ê³µê³  ë°œê²¬</p>
        </div>
        <div>
          <h2>ğŸ”¥ ì£¼ìš” ê³µê³  ë¯¸ë¦¬ë³´ê¸° (ìµœëŒ€ 10ê°œ)</h2>
          {html_table}
        </div>
        <div style="text-align:center; margin:30px 0;">
          <a href="{pages_url}" style="display:inline-block; padding:12px 20px; background:#3498db; color:#fff; text-decoration:none; border-radius:6px;">ğŸŒ ì „ì²´ ê³µê³  ë³´ê¸°</a>
        </div>
        <p style="font-size:12px; color:#888;">ğŸ¤– Python ìë™í™” ì‹œìŠ¤í…œì´ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤ (CSV ë¯¸ì²¨ë¶€)</p>
        </body></html>
        """

        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = receiver_email
        msg['Subject'] = subject
        msg.attach(MIMEText(html_body, 'html', 'utf-8'))

        try:
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, app_password)
            server.send_message(msg)
            server.quit()
            print("ğŸ“§ ì´ë©”ì¼ ì „ì†¡ ì™„ë£Œ! (CSV ì²¨ë¶€ ì—†ìŒ)")
        except Exception as e:
            print(f"âŒ ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    crawler = SaraminCrawler()

    # 1) í¬ë¡¤ë§
    df = crawler.crawl_all(sleep_sec=0.6, page_limit=None)
    if df.empty:
        print("ì¢…ë£Œ: ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
        exit()

    # 2) CSV ì €ì¥ (GitHub Pages ì—…ë°ì´íŠ¸/ë°±ì—…ìš© - ë©”ì¼ì—ëŠ” ì²¨ë¶€í•˜ì§€ ì•ŠìŒ)
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_csv = f"saramin_results_{ts}.csv"
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {len(df)} rows â†’ {out_csv}")

    # 3) HTML ì €ì¥ (/docs í´ë”)
    docs_dir = Path("docs")
    html_path = docs_dir / "saramin_results_latest.html"
    crawler.build_html_page(df, str(html_path))

    # 4) GitHub Pages URL (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
    pages_url = "https://pkpjs.github.io/test/saramin_results_latest.html"

    # 5) ì´ë©”ì¼ ë°œì†¡ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ì½ê¸°)
    EMAIL_SENDER   = os.environ.get("EMAIL_SENDER")
    EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")
    EMAIL_PASSWORD = os.environ.get("EMAIL_APP_PASSWORD")
    if not all([EMAIL_SENDER, EMAIL_RECEIVER, EMAIL_PASSWORD]):
        print("âŒ í™˜ê²½ ë³€ìˆ˜(EMAIL_SENDER / EMAIL_RECEIVER / EMAIL_APP_PASSWORD)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ğŸ”” GitHub Secrets ë˜ëŠ” ì‹¤í–‰ í™˜ê²½ì˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)

    # 6) ì´ë©”ì¼ ë°œì†¡ (CSV ì²¨ë¶€ ì—†ìŒ)
    jobs_list = df.to_dict(orient="records")
    crawler.send_email(
        jobs=jobs_list,
        sender_email=EMAIL_SENDER,
        app_password=EMAIL_PASSWORD,
        receiver_email=EMAIL_RECEIVER,
        pages_url=pages_url
    )

    print(f"ğŸ”— ì „ì²´ ê³µê³  í˜ì´ì§€ ì£¼ì†Œ: {pages_url}")
