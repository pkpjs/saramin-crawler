name: Run Saramin Crawler

on:
  workflow_dispatch:      # ì‚¬ìš©ìê°€ Actionsì—ì„œ ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥
  schedule:
    - cron: "0 0 * * *"   # ë§¤ì¼ í•œêµ­ì‹œê°„ ì˜¤ì „ 9ì‹œ ì‹¤í–‰ (UTC+9 â†’ 0ì‹œ)

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write    # docs í´ë”ì— HTML ì €ì¥ í›„ ì»¤ë°‹í•˜ê¸° ìœ„í•¨

    env:
      EMAIL_SENDER: ${{ secrets.EMAIL_SENDER }}
      EMAIL_RECEIVER: ${{ secrets.EMAIL_RECEIVER }}
      EMAIL_APP_PASSWORD: ${{ secrets.EMAIL_APP_PASSWORD }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0   # ì „ì²´ ê¸°ë¡ ê°€ì ¸ì˜¤ê¸° (ë®ì–´ì“°ê¸°ì— í•„ìš”)

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install requests beautifulsoup4 pandas google-auth google-auth-oauthlib google-api-python-client

      - name: Run Saramin Crawler
        run: python test.py   # íŒŒì¼ ì´ë¦„ì´ ë‹¤ë¥´ë©´ ìˆ˜ì •í•´ì£¼ì„¸ìš”

      - name: Commit and Push HTML & CSV
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/*.html *.csv || true
          if git diff --cached --quiet; then
            echo "ë³€ê²½ëœ íŒŒì¼ ì—†ìŒ. ì»¤ë°‹ ê±´ë„ˆëœ€."
          else
            git commit -m "ğŸ”„ Auto update saramin_results_latest.html & CSV ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
            git push origin main
          fi
