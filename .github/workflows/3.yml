name: Run Saramin Crawler

on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 * * *"  # í•œêµ­ì‹œê°„ ì˜¤ì „ 9ì‹œ ìë™ ì‹¤í–‰ (UTC 0ì‹œ ê¸°ì¤€)

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    env:
      EMAIL_SENDER: ${{ secrets.EMAIL_SENDER }}
      EMAIL_RECEIVER: ${{ secrets.EMAIL_RECEIVER }}
      EMAIL_APP_PASSWORD: ${{ secrets.EMAIL_APP_PASSWORD }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install requests beautifulsoup4 pandas google-auth google-auth-oauthlib google-api-python-client

      - name: Run Saramin Crawler
        run: python test.py

      - name: Remove old CSV files
        run: |
          echo "ğŸ§¹ Removing yesterday's CSV files..."
          find . -maxdepth 1 -type f -name "saramin_results_*.csv" ! -name "saramin_results_$(date -u +'%Y%m%d')*.csv" -delete || true
          echo "âœ… Cleanup done."

      - name: Commit and Push HTML & CSV
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          echo "âœ… Pulling latest changes with rebase..."
          git pull origin main --rebase || true

          git add docs/*.html *.csv || true

          if git diff --cached --quiet; then
            echo "âœ… No changes detected. Skipping commit."
          else
            git commit -m "ğŸ”„ Auto update saramin_results_latest.html & CSV ($(date -u +'%Y-%m-%dT%H:%M:%SZ'))"
            git push origin main
            echo "âœ… Successfully pushed changes."
          fi
