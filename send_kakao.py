# -*- coding: utf-8 -*-
import os, re, json, requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta, timezone

# ===== ê¸°ë³¸ ì„¤ì • =====
KST = timezone(timedelta(hours=9))
REST_API_KEY  = os.getenv("KAKAO_REST_API_KEY")
REFRESH_TOKEN = os.getenv("KAKAO_REFRESH_TOKEN")
PAGES_URL     = os.getenv("PAGES_URL", "https://pkpjs.github.io/test/saramin_results_latest.html")
HTML_PATH     = "docs/saramin_results_latest.html"
STATE_PATH    = "docs/last_rec_ids.json"   # ì‹ ê·œ ê°ì§€ìš© ì €ì¥ íŒŒì¼
SARAMIN_BASE  = "https://www.saramin.co.kr"

# ì§€ì—­ ì„ í˜¸ (ì˜µì…˜). ì˜ˆ: "ëŒ€ì „,ëŒ€êµ¬,ìˆ˜ë„ê¶Œ"
PREFERRED_REGIONS = [s.strip() for s in os.getenv("PREFERRED_REGIONS","").split(",") if s.strip()]

# ìš°ì„ ìˆœìœ„: D(ë§ˆê°) > C(ì‹ ê·œ) > E(ì§€ì—­) > B(ê¸°ì—…) > A(ì—°ë´‰)
DEADLINE_IMMINENT_3D = 50
DEADLINE_IMMINENT_7D = 40
DEADLINE_NONE        = 10
FRESH_NEW            = 30
FRESH_OLD            = -10
REGION_HIT           = 20   # ì„ í˜¸ì§€ì—­ ìˆì„ ë•Œë§Œ ì ìš©
FIRM_BIG             = 15
FIRM_MID             = 10
SALARY_GOOD          = 5

BIG_FIRM_HINTS = ["ëŒ€ê¸°ì—…", "ê³µê¸°ì—…", "ê³µì‚¬", "ê³µë‹¨", "ê·¸ë£¹",
                  "ì‚¼ì„±","LG","í˜„ëŒ€","ë¡¯ë°","í•œí™”","SK","ì¹´ì¹´ì˜¤","ë„¤ì´ë²„","KT","í¬ìŠ¤ì½”"]
MID_FIRM_HINTS = ["ì¤‘ê²¬","ê°•ì†Œ","ìš°ëŸ‰"]

def refresh_access_token() -> str:
    url = "https://kauth.kakao.com/oauth/token"
    data = {"grant_type": "refresh_token", "client_id": REST_API_KEY, "refresh_token": REFRESH_TOKEN}
    r = requests.post(url, data=data, timeout=20); r.raise_for_status()
    js = r.json()
    if "access_token" not in js:
        raise RuntimeError(f"í† í° ê°±ì‹  ì‹¤íŒ¨: {js}")
    return js["access_token"]

def load_html_text() -> str:
    try:
        with open(HTML_PATH, "r", encoding="utf-8", errors="ignore") as f:
            return f.read()
    except FileNotFoundError:
        r = requests.get(PAGES_URL, timeout=20); r.raise_for_status()
        return r.text

def parse_deadline(text: str):
    if not text: return None
    t = text.strip()
    if any(k in t for k in ["ìƒì‹œ","ìˆ˜ì‹œ","ì±„ìš©ì‹œ"]): return None
    m = re.search(r'(\d{1,2})\s*[./-]\s*(\d{1,2})', t) or re.search(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', t)
    if not m: return None
    month, day = int(m.group(1)), int(m.group(2))
    now = datetime.now(KST); year = now.year
    d = datetime(year, month, day, tzinfo=KST)
    if d < now - timedelta(days=180): d = datetime(year+1, month, day, tzinfo=KST)
    return d

def days_to_deadline(d):
    if not d: return None
    return (d.date() - datetime.now(KST).date()).days

def extract_items():
    """
    í‘œ ì»¬ëŸ¼ ê°€ì •:
    ë§í¬/ì œëª© | íšŒì‚¬ | ìœ„ì¹˜ | ê²½ë ¥ | í•™ë ¥ | ë§ˆê°ì¼ | ë°”ë¡œê°€ê¸° | (ì—°ë´‰/ê¸‰ì—¬ ìˆìœ¼ë©´ ê°€ì‚°)
    """
    html = load_html_text()
    soup = BeautifulSoup(html, "lxml")
    table = soup.find("table")
    if not table: return [], 0

    headers = [th.get_text(strip=True) for th in table.select("thead tr th")]
    if not headers:
        first = table.find("tr")
        if first: headers = [th.get_text(strip=True) for th in first.find_all(["th","td"])]

    rows = table.select("tbody tr") or table.find_all("tr")[1:]
    total = len(rows)

    def idx(*names):
        for n in names:
            if n in headers: return headers.index(n)
        return None

    i_link   = idx("ë§í¬","ì œëª©","title")
    i_company= idx("íšŒì‚¬","company")
    i_loc    = idx("ìœ„ì¹˜","location")
    i_job    = idx("ì§ë¬´","job")
    i_dead   = idx("ë§ˆê°ì¼","ë§ˆê°","deadline")
    i_direct = idx("ë°”ë¡œê°€ê¸°")
    i_salary = idx("ì—°ë´‰","ê¸‰ì—¬","salary")

    items = []
    for tr in rows:
        tds = tr.find_all("td")
        if not tds: continue

        title, url = "", ""
        if i_link is not None and i_link < len(tds):
            a = tds[i_link].find("a", href=True)
            if a:
                title = a.get_text(strip=True)
                href  = a["href"].strip()
                url   = href if href.startswith("http") else urljoin(SARAMIN_BASE, href)
            else:
                title = tds[i_link].get_text(strip=True)

        if not url and i_direct is not None and i_direct < len(tds):
            a2 = tds[i_direct].find("a", href=True)
            if a2:
                href2 = a2["href"].strip()
                url = href2 if href2.startswith("http") else urljoin(SARAMIN_BASE, href2)

        company = tds[i_company].get_text(strip=True) if i_company is not None and i_company < len(tds) else ""
        loc     = tds[i_loc].get_text(strip=True)     if i_loc     is not None and i_loc     < len(tds) else ""
        job     = tds[i_job].get_text(strip=True)     if i_job     is not None and i_job     < len(tds) else ""
        deadraw = tds[i_dead].get_text(strip=True)    if i_dead    is not None and i_dead    < len(tds) else ""
        salary  = tds[i_salary].get_text(strip=True)  if i_salary  is not None and i_salary  < len(tds) else ""

        # rec_idx ì¶”ì¶œ (ì‹ ê·œì„± íŒì •ìš©)
        rec_idx = None
        if url:
            m = re.search(r"rec_idx=(\d+)", url)
            if m: rec_idx = m.group(1)

        items.append({
            "title": title or "(ì œëª© ì—†ìŒ)",
            "company": company,
            "location": loc,
            "job": job,
            "deadline_text": deadraw,
            "deadline": parse_deadline(deadraw),
            "salary": salary,
            "url": url or PAGES_URL,
            "rec_idx": rec_idx
        })
    return items, total

def load_last_rec_ids():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except FileNotFoundError:
        return set()
    except Exception:
        return set()

def save_current_rec_ids(items):
    recs = [x["rec_idx"] for x in items if x.get("rec_idx")]
    try:
        os.makedirs(os.path.dirname(STATE_PATH), exist_ok=True)
        with open(STATE_PATH, "w", encoding="utf-8") as f:
            json.dump(recs, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print("[WARN] rec_ids ì €ì¥ ì‹¤íŒ¨:", e)

def firm_score(name: str):
    n = (name or "").lower()
    big = any(k.lower() in n for k in BIG_FIRM_HINTS)
    mid = any(k.lower() in n for k in MID_FIRM_HINTS)
    if big: return FIRM_BIG
    if mid: return FIRM_MID
    return 0

def salary_score(text: str):
    if not text: return 0
    if "í˜‘ì˜" in text: return 0
    nums = [int(x) for x in re.findall(r'\d{3,4}', text)]
    if not nums: return 0
    # 3500 ì´ìƒ ì–¸ê¸‰ë˜ë©´ ì†Œí­ ê°€ì‚°
    return SALARY_GOOD if max(nums) >= 3500 else 0

def region_score(loc: str):
    if not PREFERRED_REGIONS: return 0
    l = (loc or "").lower()
    return REGION_HIT if any(p.lower() in l for p in PREFERRED_REGIONS) else 0

def deadline_score(deadline):
    d = days_to_deadline(deadline)
    if d is None: return DEADLINE_NONE
    if d <= 3:    return DEADLINE_IMMINENT_3D
    if d <= 7:    return DEADLINE_IMMINENT_7D
    return max(0, 30 - min(d, 30))  # ë©€ìˆ˜ë¡ ê°ì†Œ(ìµœëŒ€ 30â†’0)

def freshness_score(item, last_ids: set):
    rec = item.get("rec_idx")
    if rec and rec not in last_ids: return FRESH_NEW
    return FRESH_OLD

def score_item(item, last_ids: set):
    s = 0
    s += deadline_score(item["deadline"])                  # D
    s += freshness_score(item, last_ids)                   # C
    s += region_score(item["location"])                    # E (ì—†ìœ¼ë©´ 0)
    s += firm_score(item["company"])                       # B
    s += salary_score(item["salary"])                      # A
    return s

def rank_top10(items):
    last_ids = load_last_rec_ids()
    for it in items:
        it["score"] = score_item(it, last_ids)
    items.sort(key=lambda x: x["score"], reverse=True)
    top10 = items[:10]
    save_current_rec_ids(items)  # ì´ë²ˆ íšŒì°¨ ê¸°ë¡
    return top10

# ===== ì „ì†¡ =====
def send_feed(access_token, rank, item):
    # ë³¸ë¬¸: íšŒì‚¬ / ìœ„ì¹˜ | ì§ë¬´ / ë§ˆê°
    parts = []
    if item["company"]: parts.append(item["company"])
    line2 = " | ".join([t for t in [item["location"], item["job"]] if t])
    if line2: parts.append(line2)
    if item["deadline_text"]: parts.append(f"ë§ˆê°: {item['deadline_text']}")
    desc = "\n".join(parts)

    template_object = {
        "object_type": "feed",
        "content": {
            "title": f"{rank}ìœ„ ({item['score']}ì ) | {item['title']}",
            "description": desc,
            "link": {"web_url": item["url"], "mobile_web_url": item["url"]},
        },
        "buttons": [
            {"title": "ê³µê³  ë°”ë¡œê°€ê¸° ğŸ”—", "link": {"web_url": item["url"], "mobile_web_url": item["url"]}}
        ]
    }
    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {"template_object": json.dumps(template_object, ensure_ascii=False)}
    r = requests.post(url, headers=headers, data=data, timeout=20)
    js = r.json()
    if js.get("result_code") != 0:
        raise RuntimeError(f"{rank}ìœ„ ì „ì†¡ ì‹¤íŒ¨: {js}")

def send_header(access_token, total):
    today = datetime.now(KST).strftime("%Y-%m-%d")
    text = f"ğŸ“… {today} AI ì¶”ì²œ TOP 10 (ë§ˆê° ì„ë°• > ì‹ ê·œ > ì§€ì—­ > ê¸°ì—… > ì—°ë´‰)\nì›ë³¸ {total}ê±´ì—ì„œ ì„ ë³„/ì ìˆ˜í™”í–ˆìŠµë‹ˆë‹¤."
    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {"template_object": json.dumps({
        "object_type": "text",
        "text": text,
        "link": {"web_url": PAGES_URL, "mobile_web_url": PAGES_URL},
        "button_title": "ì „ì²´ ê³µê³  ë³´ê¸°"
    }, ensure_ascii=False)}
    requests.post(url, headers=headers, data=data, timeout=20)

def send_tail(access_token):
    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {"template_object": json.dumps({
        "object_type": "feed",
        "content": {
            "title": "ì „ì²´ ì±„ìš© ê³µê³  í•œ ë²ˆì— ë³´ê¸°",
            "description": "GitHub Pagesì—ì„œ ìµœì‹  ì „ì²´ ëª©ë¡ í™•ì¸",
            "link": {"web_url": PAGES_URL, "mobile_web_url": PAGES_URL},
        },
        "buttons": [{"title": "ì „ì²´ë³´ê¸° ğŸ”—", "link": {"web_url": PAGES_URL, "mobile_web_url": PAGES_URL}}]
    }, ensure_ascii=False)}
    requests.post(url, headers=headers, data=data, timeout=20)

def main():
    access_token = refresh_access_token()
    items_all, total = extract_items()
    if not items_all:
        print("âŒ ë°ì´í„° ì—†ìŒ"); return
    top10 = rank_top10(items_all)

    send_header(access_token, total)
    for i, it in enumerate(top10, start=1):
        send_feed(access_token, i, it)
    send_tail(access_token)
    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {len(top10)}ê°œ")

if __name__ == "__main__":
    main()