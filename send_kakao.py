# -*- coding: utf-8 -*-
import os, re, json, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# ===== ê¸°ë³¸ ì„¤ì • =====
KST = timezone(timedelta(hours=9))
REST_API_KEY = os.getenv("KAKAO_REST_API_KEY")
REFRESH_TOKEN = os.getenv("KAKAO_REFRESH_TOKEN")
PAGES_URL = os.getenv("PAGES_URL", "https://pkpjs.github.io/test/saramin_results_latest.html")
HTML_PATH = "docs/saramin_results_latest.html"
STATE_PATH = "docs/last_rec_ids.json"
SARAMIN_BASE = "https://www.saramin.co.kr"

# ===== ì ìˆ˜ ê°€ì¤‘ì¹˜ =====
DEADLINE_IMMINENT_3D = 50
DEADLINE_IMMINENT_7D = 40
DEADLINE_NONE = 10
FRESH_NEW = 30
FRESH_OLD = -10
FIRM_BIG = 15
FIRM_MID = 10
SALARY_GOOD = 5

BIG_FIRM_HINTS = ["ëŒ€ê¸°ì—…","ê³µê¸°ì—…","ê³µì‚¬","ê³µë‹¨","ê·¸ë£¹","ì‚¼ì„±","LG","í˜„ëŒ€","ë¡¯ë°","í•œí™”","SK","ì¹´ì¹´ì˜¤","ë„¤ì´ë²„","KT","í¬ìŠ¤ì½”"]
MID_FIRM_HINTS = ["ì¤‘ê²¬","ê°•ì†Œ","ìš°ëŸ‰"]

# ===== Kakao í† í° ê°±ì‹  =====
def refresh_access_token() -> str:
    url = "https://kauth.kakao.com/oauth/token"
    data = {"grant_type": "refresh_token", "client_id": REST_API_KEY, "refresh_token": REFRESH_TOKEN}
    r = requests.post(url, data=data, timeout=20)
    r.raise_for_status()
    js = r.json()
    if "access_token" not in js:
        raise RuntimeError(f"í† í° ê°±ì‹  ì‹¤íŒ¨: {js}")
    return js["access_token"]

# ===== HTML ë¡œë“œ ë° ì •ì œ =====
def load_html_text() -> str:
    try:
        with open(HTML_PATH, "r", encoding="utf-8", errors="ignore") as f:
            html = f.read()
    except FileNotFoundError:
        r = requests.get(PAGES_URL, timeout=20)
        r.raise_for_status()
        html = r.text

    # âœ… <style>, <script> ì œê±° (íŒŒì‹± ë°©í•´ ë°©ì§€)
    cleaned = re.sub(r"<style.*?>.*?</style>", "", html, flags=re.DOTALL | re.IGNORECASE)
    cleaned = re.sub(r"<script.*?>.*?</script>", "", cleaned, flags=re.DOTALL | re.IGNORECASE)
    return cleaned

# ===== ë§ˆê°ì¼ íŒŒì‹± =====
def parse_deadline(text: str):
    if not text:
        return None
    t = text.strip()
    if any(k in t for k in ["ìƒì‹œ","ìˆ˜ì‹œ","ì±„ìš©ì‹œ","ìƒì‹œì±„ìš©","ìˆ˜ì‹œì±„ìš©"]):
        return None
    m = re.search(r'(\d{1,2})\s*[./-]\s*(\d{1,2})', t) or re.search(r'(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', t)
    if not m:
        return None
    month, day = int(m.group(1)), int(m.group(2))
    now = datetime.now(KST)
    year = now.year
    d = datetime(year, month, day, tzinfo=KST)
    if d < now - timedelta(days=180):
        d = datetime(year+1, month, day, tzinfo=KST)
    return d

def days_to_deadline(d):
    if not d: return None
    return (d.date() - datetime.now(KST).date()).days

def format_deadline_display(deadline_dt, raw_text: str) -> str:
    t = (raw_text or "").strip()
    low = t.replace(" ", "").lower()
    if any(k in low for k in ["ì˜¤ëŠ˜ë§ˆê°","today"]): return "ì˜¤ëŠ˜ë§ˆê°"
    if any(k in low for k in ["ë‚´ì¼ë§ˆê°","tomorrow"]): return "ë‚´ì¼ë§ˆê°"
    if any(k in t for k in ["ìƒì‹œ","ìˆ˜ì‹œ","ì±„ìš©ì‹œ","ìƒì‹œì±„ìš©","ìˆ˜ì‹œì±„ìš©"]): return "ìƒì‹œì±„ìš©"
    if deadline_dt:
        dday = days_to_deadline(deadline_dt)
        mmdd = deadline_dt.strftime("%m/%d")
        if dday is not None:
            if dday < 0:  return f"ë§ˆê° {mmdd} (D+{abs(dday)})"
            if dday == 0: return f"ë§ˆê° {mmdd} (D-0)"
            return f"ë§ˆê° {mmdd} (D-{dday})"
        return f"ë§ˆê° {mmdd}"
    return t or ""

# ===== ê³µê³  ì¶”ì¶œ =====
def extract_items():
    html = load_html_text()
    soup = BeautifulSoup(html, "lxml")

    table = soup.find("table")
    if not table:
        # âœ… ë°±ì—… ë¡œì§: <h2> ì´í›„ <table> ê°•ì œ íƒìƒ‰
        for tag in soup.find_all("table"):
            table = tag
            break
        if not table:
            print("âŒ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. HTML êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return [], 0

    headers = [th.get_text(strip=True) for th in table.find_all("th")]
    rows = table.find_all("tr")[1:]
    total = len(rows)

    def idx(*names):
        for n in names:
            if n in headers:
                return headers.index(n)
        return None

    i_title   = idx("ì œëª©")
    i_company = idx("íšŒì‚¬","company")
    i_loc     = idx("ìœ„ì¹˜","location")
    i_job     = idx("ì§ë¬´","job")
    i_dead    = idx("ë§ˆê°ì¼","ë§ˆê°","deadline")
    i_salary  = idx("ì—°ë´‰","ê¸‰ì—¬","salary")
    i_link_col = idx("ë§í¬")

    items = []
    for tr in rows:
        tds = tr.find_all("td")
        if not tds: continue

        title, url, rec_idx = "", "", None

        if i_title is not None and i_title < len(tds):
            title = tds[i_title].get_text(strip=True)

        if i_link_col is not None and i_link_col < len(tds):
            a = tds[i_link_col].find("a", href=True)
            if a:
                full_url = a["href"].strip()
                m = re.search(r"rec_idx=(\d+)", full_url)
                if m:
                    rec_idx = m.group(1)
                    url = f"https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx={rec_idx}"
                else:
                    url = full_url

        company = tds[i_company].get_text(strip=True) if i_company is not None and i_company < len(tds) else ""
        loc     = tds[i_loc].get_text(strip=True) if i_loc is not None and i_loc < len(tds) else ""
        job     = tds[i_job].get_text(strip=True) if i_job is not None and i_job < len(tds) else "(ì§ë¬´ì •ë³´ì—†ìŒ)"
        deadraw = tds[i_dead].get_text(strip=True) if i_dead is not None and i_dead < len(tds) else ""
        salary  = tds[i_salary].get_text(strip=True) if i_salary is not None and i_salary < len(tds) else ""

        deadline_dt = parse_deadline(deadraw)
        deadline_disp = format_deadline_display(deadline_dt, deadraw)

        items.append({
            "title": title or "(ì œëª© ì—†ìŒ)",
            "company": company,
            "location": loc,
            "job": job,
            "deadline_text": deadraw,
            "deadline": deadline_dt,
            "deadline_disp": deadline_disp,
            "salary": salary,
            "url": url or PAGES_URL,
            "rec_idx": rec_idx
        })
    return items, total

# ===== rec_idx ê´€ë¦¬ =====
def load_last_rec_ids():
    try:
        with open(STATE_PATH, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except Exception:
        return set()

def save_current_rec_ids(items):
    recs = [x["rec_idx"] for x in items if x.get("rec_idx")]
    try:
        os.makedirs(os.path.dirname(STATE_PATH), exist_ok=True)
        with open(STATE_PATH, "w", encoding="utf-8") as f:
            json.dump(recs, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

# ===== ì ìˆ˜ ê³„ì‚° =====
def deadline_score(deadline):
    d = days_to_deadline(deadline)
    if d is None: return DEADLINE_NONE
    if d <= 3:    return DEADLINE_IMMINENT_3D
    if d <= 7:    return DEADLINE_IMMINENT_7D
    return max(0, 30 - min(d, 30))

def freshness_score(item, last_ids: set):
    rec = item.get("rec_idx")
    return FRESH_NEW if rec and rec not in last_ids else FRESH_OLD

def firm_score(name: str):
    n = (name or "").lower()
    if any(k.lower() in n for k in BIG_FIRM_HINTS): return FIRM_BIG
    if any(k.lower() in n for k in MID_FIRM_HINTS): return FIRM_MID
    return 0

def salary_score(text: str):
    if not text: return 0
    if "í˜‘ì˜" in text: return 0
    nums = [int(x) for x in re.findall(r'\d{3,4}', text)]
    if nums and max(nums) >= 3500:
        return SALARY_GOOD
    return 0

def score_item(item, last_ids: set):
    return (
        deadline_score(item["deadline"]) +
        freshness_score(item, last_ids) +
        firm_score(item["company"]) +
        salary_score(item["salary"])
    )

def rank_top(items, k=5):
    last_ids = load_last_rec_ids()
    for it in items:
        it["score"] = score_item(it, last_ids)
    items.sort(key=lambda x: x["score"], reverse=True)
    topk = items[:k]
    save_current_rec_ids(items)
    return topk

# ===== ì¹´ì¹´ì˜¤í†¡ ì „ì†¡ =====
def send_text(access_token: str, text: str):
    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {"Authorization": f"Bearer {access_token}"}
    chunks = []
    buf = []
    cur_len = 0
    for line in text.splitlines():
        add = len(line) + 1
        if cur_len + add > 1000:
            chunks.append("\n".join(buf))
            buf, cur_len = [], 0
        buf.append(line)
        cur_len += add
    if buf:
        chunks.append("\n".join(buf))

    for i, chunk in enumerate(chunks, start=1):
        suffix = f"\n\n(#{i}/{len(chunks)})" if len(chunks) > 1 else ""
        template_object = {
            "object_type": "text",
            "text": chunk + suffix,
            "link": {"web_url": PAGES_URL, "mobile_web_url": PAGES_URL},
            "button_title": "ì „ì²´ ê³µê³  ë³´ê¸°"
        }
        data = {"template_object": json.dumps(template_object, ensure_ascii=False)}
        r = requests.post(url, headers=headers, data=data, timeout=20)
        try:
            print("ì „ì†¡ ê²°ê³¼:", r.json())
        except Exception:
            print("ì „ì†¡ ê²°ê³¼:", r.text)

# ===== ë©”ì¸ =====
def main():
    access_token = refresh_access_token()
    items_all, total = extract_items()
    if not items_all:
        print("âŒ ë°ì´í„° ì—†ìŒ")
        return

    top5 = rank_top(items_all, k=5)

    today = datetime.now(KST).strftime("%Y-%m-%d")
    lines = []
    lines.append(f"ğŸ“… {today} ê¸°ì¤€ AI ì¶”ì²œ TOP 5 ì±„ìš©ê³µê³ ")
    lines.append(f"ì´ {total}ê°œ ì¤‘ ì„ ë³„ëœ ìƒìœ„ ê³µê³ ì…ë‹ˆë‹¤.\n")

    for i, it in enumerate(top5, start=1):
        title_line = f"{i}ìœ„ ({it['score']}ì ) | {it['company']} / {it['job']} | {it['location']} | {it['deadline_disp']}"
        lines.append(title_line)
        real_link = it.get('url') or f"https://www.saramin.co.kr/zf_user/jobs/relay/view?rec_idx={it.get('rec_idx','')}"
        lines.append(f"ğŸ”— {real_link}\n")

    lines.append(f"ğŸ‘‡ ì „ì²´ ê³µê³  ë³´ê¸°:\n{PAGES_URL}")
    final_message = "\n".join(lines)
    send_text(access_token, final_message)
    print(f"âœ… ì „ì†¡ ì™„ë£Œ: {len(top5)}ê°œ í•­ëª©")

if __name__ == "__main__":
    main()
